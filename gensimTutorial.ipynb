{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'> Gensim </span>\n",
    "- Gensim - Package for Topic Modeling, text processing\n",
    "- uses LDA, LSI \n",
    "- Much wider and deeper usage than text package available in Scikit Learn , R etc\n",
    "- Gensim - handles large text file efficiently without having to load all texts in one go into RAM\n",
    "- Dictionary : Convert texts to tokens and creates unique ids for each of these tokens/words\n",
    "- Token : Words\n",
    "- Documents : Sentence, Paragraph\n",
    "- Corpus: Collection of documents as collection of words => word id+frequency in each DOCUMENT.   \n",
    "__For topic modeling we require DIctionary (for unique word) and corpus (bag of words fo each document = word_id,Freq of word_id for each document__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:brown'>GENSIM allows to read senteneces line by line without loading entire text file into RAM, yet read line by line and update DICTIONARY, as and when encounters new word</brown>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary from list of texts \n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[text for text in doc.split()] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary=corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Append new dictionary\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts2=[[text for text in doc.split()] for doc in documents_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['One', 'source', 'says', 'the', 'report', 'will', 'likely', 'conclude', 'that'], ['the', 'operation', 'was', 'carried', 'out', 'without', 'clearance', 'and'], ['transparency', 'and', 'that', 'those', 'involved', 'will', 'be', 'held'], ['responsible.', 'One', 'of', 'the', 'sources', 'acknowledged', 'that', 'the'], ['report', 'is', 'still', 'being', 'prepared', 'and', 'cautioned', 'that'], ['things', 'could', 'change.']]\n"
     ]
    }
   ],
   "source": [
    "print(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.add_documents(texts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(60 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'One': 33, 'conclude': 34, 'likely': 35, 'says': 36, 'source': 37, 'and': 38, 'carried': 39, 'clearance': 40, 'operation': 41, 'out': 42, 'without': 43, 'be': 44, 'held': 45, 'involved': 46, 'those': 47, 'transparency': 48, 'acknowledged': 49, 'responsible.': 50, 'sources': 51, 'being': 52, 'cautioned': 53, 'is': 54, 'prepared': 55, 'still': 56, 'change.': 57, 'could': 58, 'things': 59}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='Ã§olor:blue'> Create dictionary from one or more files </span>\n",
    "- Created one notepad as Sample Txt , stored in the same path where am executing this script\n",
    "- use gensim util simple process to read file line by line without loading into RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary=corpora.Dictionary(simple_preprocess(line,deacc=True) for line in open('sample.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(93 unique tokens: ['army', 'china', 'chinese', 'force', 'liberation']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read one line at a time from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReadTxtFiles(object):\n",
    "    def __init__(self,dirname):\n",
    "        self.dirname = dirname\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname,fname),encoding='latin'):\n",
    "                yield simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_text_directory ='E:/Niraj/Niraj Personal/Learnings/Machine Learning/Data/datasets/lda_sports_politics_docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(959 unique tokens: ['across', 'activity', 'although', 'and', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:purple'>Generator vs Iteration vs Iterables</span>\n",
    "- Generator : Data gets exhausted once burned thru Gen\n",
    "- Iterable  : Data gets repeated after ever iteration , its like new for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = range(1,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 10, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for val in (generator):\n",
    "    print(val),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = (word + '!' for word in ((\"baby let me iterate ya\").split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby!\n",
      "let!\n",
      "me!\n",
      "iterate!\n",
      "ya!\n"
     ]
    }
   ],
   "source": [
    "for i in generator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in generator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Beyonceiterable(object):\n",
    "    def __iter__(self):\n",
    "        for word in \"baby let me iterate over ya\".split():\n",
    "            yield word +\"!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterable =Beyonceiterable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baby!\n",
      "let!\n",
      "me!\n",
      "iterate!\n",
      "over!\n",
      "ya!\n"
     ]
    }
   ],
   "source": [
    "for i in iterable:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listname=('nir','pan','pagla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myiter=iter(listname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nir\n"
     ]
    }
   ],
   "source": [
    "print(next(myiter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'> Create BOW (Bag of Words) </span>\n",
    "- collect tokens\n",
    "- pass them to doc2bow function of Dictionary method by corpoa\n",
    "- Creating 'BOW' is Gensim's Document-Term matrix => It creates matrix with information about word, frequncy and document number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "my_docs =[\"who let's the dog out but who\",\"Who?Who?Who?Who?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize documents\n",
    "my_tokens=[simple_preprocess(doc) for doc in my_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['who', 'let', 'the', 'dog', 'out', 'but', 'who'],\n",
       " ['who', 'who', 'who', 'who']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the corpus\n",
    "# Create object for dictionary\n",
    "mydict=corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mycorpus=[mydict.doc2bow(doc,allow_update=True) for doc in my_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['who', 'let', 'the', 'dog', 'out', 'but', 'who'], ['who', 'who', 'who', 'who']]\n"
     ]
    }
   ],
   "source": [
    "print(my_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: ['but', 'dog', 'let', 'out', 'the']...)\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "print(corpora.Dictionary(my_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict1=corpora.Dictionary(my_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but\n",
      "dog\n",
      "let\n",
      "out\n",
      "the\n",
      "who\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mydict)):\n",
    "    print(mydict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)], [(5, 4)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('but', 1), ('dog', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 2)],\n",
       " [('who', 4)]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to intepret and decode\n",
    "[[(mydict[id],count)for id,count in doc]for doc in mycorpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Create BOW corpus from TEXT Files</span>\n",
    "- Design a class \n",
    "- init method with path and dictionary as objects\n",
    "- iter as iterative to read file and lines iteratively , one line at a time, meaning no loading to the RAM in one GO\n",
    "- instantiate the class and create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoWCorpus(object):\n",
    "    def __init__(self,path,dictionary):\n",
    "        self.filepath=path\n",
    "        self.dictionary=dictionary\n",
    "    def __iter__(self):\n",
    "        global mydict #OPTIONAl ; only if updating source dictionary\n",
    "        for line in smart_open(self.filepath):\n",
    "            # Tokenize\n",
    "            tokenized_list=simple_preprocess(line,deacc=True)\n",
    "            \n",
    "            #create bag of words\n",
    "            bow=self.dictionary.doc2bow(tokenized_list,allow_update=True)\n",
    "            \n",
    "            #update source dictionary\n",
    "            mydict.merge_with(self.dictionary)\n",
    "            \n",
    "            #return bow\n",
    "            yield bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WhatsApp Chat msg of Sood\n",
    "tokenlized_wa = [simple_preprocess(line,deacc=True) for line in open('SoodWAChat090520.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc_wa= [wd for wd in tokenlized_wa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the Dictionary object\n",
    "mydict=corpora.Dictionary()\n",
    "doc_proccessed=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare bag of word corpus of Sood WA Chat\n",
    "bow_corpus_sood= BoWCorpus('SoodWAChat090520.txt',mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Corpus\n",
    "bow_corpus= BoWCorpus('sample.txt',mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(0 unique tokens: [])\n"
     ]
    }
   ],
   "source": [
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 2), (17, 1), (18, 3), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1)]\n",
      "[(1, 2), (2, 2), (22, 1), (30, 2), (33, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1)]\n",
      "[(18, 1), (19, 1), (30, 5), (31, 1), (33, 2), (35, 1), (45, 1), (52, 1), (56, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 3), (84, 1), (85, 1), (86, 1), (87, 2), (88, 1), (89, 4), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 2), (100, 1), (101, 1), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1), (113, 1), (114, 1), (115, 2), (116, 1)]\n",
      "[(16, 1), (18, 2), (20, 1), (29, 1), (30, 4), (33, 1), (80, 1), (89, 1), (104, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1)]\n",
      "[(1, 3), (15, 1), (18, 1), (19, 1), (30, 4), (33, 1), (35, 5), (36, 1), (44, 2), (50, 2), (52, 1), (56, 1), (76, 1), (77, 2), (87, 1), (89, 1), (101, 1), (104, 1), (110, 1), (112, 1), (118, 1), (119, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 2), (139, 1), (140, 1), (141, 3), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 3), (159, 1), (160, 1), (161, 1), (162, 2), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 3), (169, 1), (170, 1), (171, 1), (172, 1), (173, 3), (174, 1), (175, 1), (176, 1), (177, 1), (178, 3), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1)]\n",
      "[(1, 2), (15, 2), (18, 1), (19, 2), (20, 1), (30, 7), (33, 5), (35, 1), (36, 1), (37, 1), (50, 1), (52, 1), (53, 1), (54, 1), (76, 1), (78, 2), (87, 1), (88, 1), (89, 1), (99, 1), (103, 1), (115, 1), (132, 1), (137, 1), (179, 1), (187, 1), (189, 1), (190, 3), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 2), (206, 1), (207, 1), (208, 1), (209, 1), (210, 2), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 2), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 2), (233, 1), (234, 1)]\n",
      "[(1, 3), (11, 1), (13, 1), (15, 1), (30, 1), (33, 2), (35, 2), (36, 1), (37, 1), (43, 1), (44, 2), (58, 1), (76, 1), (80, 1), (89, 3), (115, 1), (123, 1), (142, 1), (162, 1), (173, 2), (176, 1), (180, 1), (192, 1), (215, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 2), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1)]\n",
      "[(0, 1), (2, 1), (15, 1), (18, 1), (30, 2), (35, 2), (46, 1), (72, 1), (78, 1), (80, 1), (89, 2), (138, 1), (141, 2), (188, 1), (238, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 2), (276, 2), (277, 1), (278, 1), (279, 1), (280, 1), (281, 3), (282, 1)]\n",
      "[(1, 2), (15, 4), (18, 1), (20, 2), (29, 2), (30, 3), (33, 2), (35, 2), (36, 1), (37, 1), (50, 2), (52, 3), (58, 2), (72, 1), (88, 1), (89, 4), (114, 1), (115, 1), (118, 1), (123, 1), (128, 1), (141, 1), (151, 1), (170, 1), (202, 1), (248, 1), (283, 1), (284, 1), (285, 2), (286, 2), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 2), (294, 1), (295, 1), (296, 1), (297, 1), (298, 2), (299, 1), (300, 3), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 2), (312, 1), (313, 1), (314, 1), (315, 1), (316, 1), (317, 1), (318, 2)]\n",
      "[(0, 1), (1, 1), (18, 1), (33, 1), (78, 1), (83, 1), (99, 1), (103, 1), (305, 1), (319, 1), (320, 1), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 1)]\n",
      "[(3, 1), (5, 1), (7, 1), (11, 1), (12, 1), (17, 1), (18, 2), (20, 1), (24, 1), (27, 1), (29, 1), (30, 5), (32, 1), (33, 2), (76, 1), (83, 1), (149, 1), (264, 1), (299, 1), (321, 1), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1)]\n"
     ]
    }
   ],
   "source": [
    "for line in bow_corpus_sood:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df6=pd.DataFrame(bow_corpus_sood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_name=[[(mydict[id],count) for id,count in line]for line in bow_corpus_sood]\n",
    "#We can display corpus_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents=  [\"This is the first line\",\n",
    "             \"This is the second sentence\",\n",
    "             \"This third document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict=corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_list= [simple_preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary=mydict.from_documents(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(9 unique tokens: ['first', 'is', 'line', 'the', 'this']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Corpus\n",
    "corpus=[dictionary.doc2bow(token) for token in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(4, 1), (7, 1), (8, 1)]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('first', 1), ('is', 1), ('line', 1), ('the', 1), ('this', 1)],\n",
       " [('is', 1), ('the', 1), ('this', 1), ('second', 1), ('sentence', 1)],\n",
       " [('this', 1), ('document', 1), ('third', 1)]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(dictionary[id],count) for id,count in corpus_list] for corpus_list in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style ='color:blue'>Enough of exercise, lets sum up the steps </span>\n",
    "\n",
    "- Documents to token\n",
    "- token to dictionary\n",
    "- documents to corpus consisting with dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Craete Disctionary and Corpus\n",
    "mydict= corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "#mydict.add_documents([['hi','bye']])\n",
    "\n",
    "#mydict=corpora.Dictionary()\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for i in range(len(mydict)):\n",
    "#    print(mydict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(1, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(4, 1), (7, 1), (8, 1)]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>How to save dictionary and corpus to the disk</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict.save('my_dictionary')\n",
    "corpora.MmCorpus.serialize('bow_corpus.nm',corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:green'> TF-IDF Matrix in Gensim </span>\n",
    "- Multiply local Term Frequency with Inverse Global Term and Normalizes result to Unit LEngth\n",
    "- Words with high frequency will get down weghed\n",
    "- Genesim uses <a href ='https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System'>SMART INFORMATION RETRIEVAL SYSTEM</a> to calculate TF IDF weigh\n",
    "- We can use 'smartirs' parameter in TFIDF model to calculate this term\n",
    "- Normalizing the CORPUS factoring local component i.e. TF (Term Frequency) and Global component i.e. IDF (Inverse Doc. frequnecy)\n",
    "- If certain words only come with high frequency in less documents the yield high value\n",
    "- If words come in all documents then yield low value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "tfidf=models.TfidfModel(corpus,smartirs='ntc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
      "[['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
      "[['document', 0.71], ['third', 0.71]]\n"
     ]
    }
   ],
   "source": [
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id],np.around(freq,decimals=2)]for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:green'>Observation above after TF-IDF</span>\n",
    "- high frequency words across documents are reduced with value such as 'is','the'\n",
    "- Words coming always such as 'this' has been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Use GENSIM downloader API to load dataset </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download 'glove-wiki-gigaword-50' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       " 'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       " 'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       " 'file_name': 'glove-wiki-gigaword-50.gz',\n",
       " 'file_size': 69182535,\n",
       " 'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       " 'num_records': 400000,\n",
       " 'parameters': {'dimension': 50},\n",
       " 'parts': 1,\n",
       " 'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       " 'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "  'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download\n",
    "w2v_model=api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.4'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('yellow', 0.8995457887649536),\n",
       " ('blue', 0.8901658654212952),\n",
       " ('green', 0.8561932444572449),\n",
       " ('black', 0.8400583267211914),\n",
       " ('purple', 0.8323202133178711),\n",
       " ('white', 0.8149363398551941),\n",
       " ('pink', 0.8148657083511353),\n",
       " ('orange', 0.8042871952056885),\n",
       " ('golden', 0.7416437864303589),\n",
       " ('colored', 0.7381109595298767)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram and Trigram model using Phraser models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'file_name': 'text8.gz',\n",
       " 'file_size': 33182058,\n",
       " 'license': 'not found',\n",
       " 'num_records': 1701,\n",
       " 'parts': 1,\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'record_format': 'list of str (tokens)'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df1=pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset=[wd for wd in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.shape(dataset[1699])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1701,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydict=corpora.Dictionary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mycorpa prepares the list of pair of numbers , 1st is the dictionary id for teh word and 2nd the frequency / occurences of such word in that partiular record\n",
    "mycorpa=[mydict.doc2bow(line) for line in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[[mydict[i],j] for i,j in mycorpa[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare BIGRAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram=gensim.models.phrases.Phrases(dataset,min_count=3,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(bigram[dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['french_revolution', 'language', 'green', 'revolution']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "print(bigram['french','revolution','language','green','revolution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian_revolution', 'niraj']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "print(bigram['russian','revolution','niraj'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Trigram : pass the o/p of bigram data to the model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "#build Trigram Model\n",
    "trigram=gensim.models.phrases.Phrases(bigram[dataset],min_count=5,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "print(np.size(bigram[dataset[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8750\n"
     ]
    }
   ],
   "source": [
    "# Construct TRIGRAM\n",
    "print(np.size(trigram[bigram[dataset[1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "print(np.size(bigram[dataset[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(bigram[dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(trigram[bigram[dataset[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bigram[dataset[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>LDA : Latent Dirichlet Allocation</span>\n",
    "- reference <a>https://www.youtube.com/watch?v=DWJYZq_fQ2A</a>\n",
    "- Used for topic modeling, developed by BIES in 2003\n",
    "- Based upon probabilistic distribution\n",
    "- Use cases : Topic for text / documentation, semantic analysis, bioinformatiocs, object localization for image\n",
    "- Document : Topic modeling context , referes to probability distribution of latent topics\n",
    "- Topic : each topic concerns different probability distribution of words\n",
    "\n",
    "__ Plate Notation __\n",
    "- Parameters \n",
    "\n",
    "$\\alpha :$ Is the parameter of Drichlet Prior on the Per document latent topic distribution  \n",
    "$\\beta  :$ Is the parameter of Drichlet prior on the per-topic word distribution  \n",
    "$\\theta{_m}$: Topic distribution for document (m)  \n",
    "$z{_m}{_n}$:Topic for nth word in document (m)  \n",
    "$w{_m}{_n}$:word (document m, word n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>Lecture by Prof. David Blei </span>\n",
    "<a> https://www.youtube.com/watch?v=FkckgwMHP2s</a>\n",
    "- Annonate Documents\n",
    "- Organize, Visualize\n",
    "- Collaborative topic modeling , people read books, texts ...we can learn from their behaviour and what they are reading\n",
    "- EM Algorithm\n",
    "\n",
    "- Visualize LDA outputs as distribute documents to the corner of TOPICS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:brown'>Generative process</span>\n",
    "- generate document with set of words matching certain topic and words distribution\n",
    "- Deteremine no of words in a document\n",
    "- Choose a TOPIC mixture for the document ; say Topic A = 20%, B= 30%, C= 50%\n",
    "- Generate the words in document by (a) Pick up a topic following above multinomial distribution (b) pick up words based upon topic multinomial distribution\n",
    "\n",
    "# <span style='color:brown'>Working backwards</span>\n",
    "- Suppose we have copus of documents\n",
    "- We wish LDA to learn a topic representation of K Topics is each document and word distributio for each topic\n",
    "- LDA back tracks from document level to identify topics that are likely to have generated the corpus\n",
    "\n",
    "## <span style='color:red'> Process</red>\n",
    "- Randomly assign 1 of K topics to each word in each document\n",
    "- For each document d\n",
    "    - Assume all topic assigmnents except for the current one is CORRECT\n",
    "    - Create two proportion (a) Proportion of words currently assigns to topic t = P(topic t|documnet d)\n",
    "    - (b) Proportion of words in all documents correspond to the topic t = P(word w|topic t)\n",
    "- Multiple these two Probs and assign w a new topic based upon teh resultant prob\n",
    "    - P(topic t|document d)*P(word w|topic t)\n",
    "- Eventuallly we reach a steady state where assignment makes sense\n",
    "\n",
    "# <span style='color:green'>CONCLUSION:</span>\n",
    "- document is the PD over topics\n",
    "- topic is PD over words\n",
    "- LDA takes number of documents , where words in each document is related\n",
    "- It then tries to figure out RECIPE as how each document could have been created\n",
    "- We can tell Model how many topics to create\n",
    "- Based upon that model/receipe we can fi d out similar documents within corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Topic Models with LDA</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 0: Import packages and stopword\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess,lemmatize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s :%(levelname)s :%(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "stop_words=stopwords.words('english')\n",
    "stop_words=stop_words+['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 1: import dataset\n",
    "data = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'file_name': 'text8.gz',\n",
       " 'file_size': 33182058,\n",
       " 'license': 'not found',\n",
       " 'num_records': 1701,\n",
       " 'parts': 1,\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'record_format': 'list of str (tokens)'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text8.Dataset"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>the</td>\n",
       "      <td>format</td>\n",
       "      <td>although</td>\n",
       "      <td>there</td>\n",
       "      <td>are</td>\n",
       "      <td>several</td>\n",
       "      <td>third</td>\n",
       "      <td>party</td>\n",
       "      <td>tools</td>\n",
       "      <td>which</td>\n",
       "      <td>...</td>\n",
       "      <td>aggression</td>\n",
       "      <td>pact</td>\n",
       "      <td>was</td>\n",
       "      <td>signed</td>\n",
       "      <td>with</td>\n",
       "      <td>provisions</td>\n",
       "      <td>that</td>\n",
       "      <td>included</td>\n",
       "      <td>consultation</td>\n",
       "      <td>arbitration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>if</td>\n",
       "      <td>either</td>\n",
       "      <td>party</td>\n",
       "      <td>disagreed</td>\n",
       "      <td>neutrality</td>\n",
       "      <td>if</td>\n",
       "      <td>either</td>\n",
       "      <td>went</td>\n",
       "      <td>to</td>\n",
       "      <td>war</td>\n",
       "      <td>...</td>\n",
       "      <td>willie</td>\n",
       "      <td>mccovey</td>\n",
       "      <td>satchel</td>\n",
       "      <td>paige</td>\n",
       "      <td>and</td>\n",
       "      <td>ozzie</td>\n",
       "      <td>smith</td>\n",
       "      <td>new</td>\n",
       "      <td>york</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>and</td>\n",
       "      <td>chicago</td>\n",
       "      <td>are</td>\n",
       "      <td>first</td>\n",
       "      <td>and</td>\n",
       "      <td>second</td>\n",
       "      <td>respectively</td>\n",
       "      <td>the</td>\n",
       "      <td>pop</td>\n",
       "      <td>band</td>\n",
       "      <td>...</td>\n",
       "      <td>two</td>\n",
       "      <td>eight</td>\n",
       "      <td>three</td>\n",
       "      <td>two</td>\n",
       "      <td>three</td>\n",
       "      <td>six</td>\n",
       "      <td>four</td>\n",
       "      <td>zero</td>\n",
       "      <td>four</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>four</td>\n",
       "      <td>five</td>\n",
       "      <td>four</td>\n",
       "      <td>eight</td>\n",
       "      <td>four</td>\n",
       "      <td>nine</td>\n",
       "      <td>five</td>\n",
       "      <td>zero</td>\n",
       "      <td>five</td>\n",
       "      <td>two</td>\n",
       "      <td>...</td>\n",
       "      <td>medal</td>\n",
       "      <td>history</td>\n",
       "      <td>historical</td>\n",
       "      <td>myths</td>\n",
       "      <td>history</td>\n",
       "      <td>of</td>\n",
       "      <td>poland</td>\n",
       "      <td>here</td>\n",
       "      <td>you</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>find</td>\n",
       "      <td>list</td>\n",
       "      <td>of</td>\n",
       "      <td>once</td>\n",
       "      <td>popular</td>\n",
       "      <td>beliefs</td>\n",
       "      <td>or</td>\n",
       "      <td>beliefs</td>\n",
       "      <td>which</td>\n",
       "      <td>are</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1         2          3           4        5             6     \\\n",
       "1696   the   format  although      there         are  several         third   \n",
       "1697    if   either     party  disagreed  neutrality       if        either   \n",
       "1698   and  chicago       are      first         and   second  respectively   \n",
       "1699  four     five      four      eight        four     nine          five   \n",
       "1700  find     list        of       once     popular  beliefs            or   \n",
       "\n",
       "         7      8      9     ...        9990     9991        9992    9993  \\\n",
       "1696    party  tools  which  ...  aggression     pact         was  signed   \n",
       "1697     went     to    war  ...      willie  mccovey     satchel   paige   \n",
       "1698      the    pop   band  ...         two    eight       three     two   \n",
       "1699     zero   five    two  ...       medal  history  historical   myths   \n",
       "1700  beliefs  which    are  ...        None     None        None    None   \n",
       "\n",
       "         9994        9995    9996      9997          9998         9999  \n",
       "1696     with  provisions    that  included  consultation  arbitration  \n",
       "1697      and       ozzie   smith       new          york         city  \n",
       "1698    three         six    four      zero          four         four  \n",
       "1699  history          of  poland      here           you          can  \n",
       "1700     None        None    None      None          None         None  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =[d for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(data[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pitenge'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "lemmatize('pitenge',allowed_tags=re.compile('NN'))[0].split(b'/')[0].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lemmatize('do not terrorize the environment',allowed_tags=re.compile('(NN|JJ|RB)'))[1].split(b'/')[0].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_processed=[]\n",
    "for i,doc in enumerate(data[:100]+tokenlized_wa):\n",
    "    doc_out=[]\n",
    "    for wd in doc:\n",
    "        if wd not in stop_words:\n",
    "            lemmatized_word=lemmatize(wd,allowed_tags=re.compile('(NN|JJ|RB)'))\n",
    "            if lemmatized_word:\n",
    "                doc_out = doc_out+[lemmatized_word[0].split(b'/')[0].decode('utf-8')]\n",
    "            else:\n",
    "                continue\n",
    "    data_processed.append(doc_out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aegis', 'zeus', 'battlefield', 'entire', 'trojan', 'army', 'wall', 'troy', 'achille', 'wrath']\n"
     ]
    }
   ],
   "source": [
    "print(data_processed[2][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hilarious'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(tokenlized_wa[0][1],allowed_tags=re.compile('(NN|JJ|RB)'))[0].split(b'/')[0].decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style='color:red'>Create dictionary and corpus as input to the LDA model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-01 21:25:55,461 :INFO :adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-08-01 21:25:55,952 :INFO :built Dictionary(40130 unique tokens: ['ability', 'able', 'abnormal', 'abolition', 'absence']...) from 111 documents (total 425825 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dict=corpora.Dictionary(data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3=pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raha',\n",
       " 'rahi',\n",
       " 'upshot',\n",
       " 'oaant',\n",
       " 'oaat',\n",
       " 'oarn',\n",
       " 'phaan',\n",
       " 'phor',\n",
       " 'shay',\n",
       " 'uill']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dict[i] for i in df3.index[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=[dict.doc2bow(line) for line in data_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(40130 unique tokens: ['ability', 'able', 'abnormal', 'abolition', 'absence']...)\n"
     ]
    }
   ],
   "source": [
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1850, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test how does corpus - word id from dctionary and frequency per document looks like\n",
    "# Remove comment to run the command\n",
    "#[print(dict[i],count) for i,count in corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1632, 2)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-01 21:25:57,563 :INFO :using asymmetric alpha [0.26219156, 0.19027454, 0.14931786, 0.12287004, 0.104381524, 0.090729296, 0.080235206]\n",
      "2020-08-01 21:25:57,581 :INFO :using symmetric eta at 0.14285714285714285\n",
      "2020-08-01 21:25:57,646 :INFO :using serial LDA version on this node\n",
      "2020-08-01 21:25:57,968 :INFO :running online LDA training, 7 topics, 10 passes over the supplied corpus of 111 documents, updating every 6000 documents, evaluating every ~0 documents, iterating 100x with a convergence threshold of 0.001000\n",
      "2020-08-01 21:25:58,024 :INFO :training LDA model using 3 processes\n",
      "2020-08-01 21:26:02,166 :INFO :PROGRESS: pass 0, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:37,534 :INFO :topic #6 (0.080): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"many\" + 0.001*\"time\" + 0.001*\"first\" + 0.001*\"year\" + 0.001*\"american\" + 0.001*\"person\" + 0.001*\"war\" + 0.001*\"new\"\n",
      "2020-08-01 21:26:37,537 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"world\" + 0.000*\"however\" + 0.000*\"new\" + 0.000*\"war\" + 0.000*\"person\"\n",
      "2020-08-01 21:26:37,540 :INFO :topic #2 (0.149): 0.001*\"also\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"world\" + 0.001*\"person\" + 0.001*\"agave\" + 0.001*\"many\" + 0.001*\"time\" + 0.001*\"year\" + 0.001*\"new\"\n",
      "2020-08-01 21:26:37,543 :INFO :topic #1 (0.190): 0.001*\"also\" + 0.001*\"american\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"year\" + 0.001*\"time\" + 0.001*\"war\" + 0.001*\"many\" + 0.001*\"person\" + 0.000*\"city\"\n",
      "2020-08-01 21:26:37,547 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.001*\"time\" + 0.001*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:37,638 :INFO :topic diff=0.180403, rho=0.125000\n",
      "2020-08-01 21:26:39,350 :INFO :-10.393 per-word bound, 1344.4 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:39,351 :INFO :PROGRESS: pass 1, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:40,311 :INFO :topic #6 (0.080): 0.003*\"also\" + 0.002*\"state\" + 0.002*\"many\" + 0.001*\"time\" + 0.001*\"first\" + 0.001*\"person\" + 0.001*\"american\" + 0.001*\"year\" + 0.001*\"war\" + 0.001*\"world\"\n",
      "2020-08-01 21:26:40,313 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:40,316 :INFO :topic #2 (0.149): 0.002*\"also\" + 0.002*\"state\" + 0.002*\"agave\" + 0.002*\"first\" + 0.001*\"person\" + 0.001*\"world\" + 0.001*\"many\" + 0.001*\"time\" + 0.001*\"year\" + 0.001*\"book\"\n",
      "2020-08-01 21:26:40,320 :INFO :topic #1 (0.190): 0.002*\"also\" + 0.002*\"american\" + 0.001*\"state\" + 0.001*\"first\" + 0.001*\"war\" + 0.001*\"city\" + 0.001*\"new\" + 0.001*\"year\" + 0.001*\"many\" + 0.001*\"time\"\n",
      "2020-08-01 21:26:40,323 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.001*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:40,327 :INFO :topic diff=0.177880, rho=0.123982\n",
      "2020-08-01 21:26:41,683 :INFO :-9.873 per-word bound, 937.5 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:41,684 :INFO :PROGRESS: pass 2, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:42,639 :INFO :topic #6 (0.080): 0.004*\"also\" + 0.002*\"state\" + 0.002*\"many\" + 0.002*\"person\" + 0.002*\"time\" + 0.002*\"first\" + 0.002*\"year\" + 0.002*\"war\" + 0.002*\"american\" + 0.002*\"world\"\n",
      "2020-08-01 21:26:42,641 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:42,645 :INFO :topic #2 (0.149): 0.003*\"also\" + 0.002*\"agave\" + 0.002*\"state\" + 0.002*\"first\" + 0.002*\"person\" + 0.002*\"many\" + 0.002*\"time\" + 0.001*\"world\" + 0.001*\"year\" + 0.001*\"book\"\n",
      "2020-08-01 21:26:42,649 :INFO :topic #1 (0.190): 0.002*\"also\" + 0.002*\"american\" + 0.002*\"state\" + 0.001*\"first\" + 0.001*\"football\" + 0.001*\"war\" + 0.001*\"city\" + 0.001*\"new\" + 0.001*\"player\" + 0.001*\"acid\"\n",
      "2020-08-01 21:26:42,652 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.001*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:42,656 :INFO :topic diff=0.152830, rho=0.123040\n",
      "2020-08-01 21:26:44,029 :INFO :-9.632 per-word bound, 793.6 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:44,030 :INFO :PROGRESS: pass 3, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:44,851 :INFO :topic #6 (0.080): 0.004*\"also\" + 0.003*\"state\" + 0.003*\"many\" + 0.002*\"person\" + 0.002*\"time\" + 0.002*\"first\" + 0.002*\"war\" + 0.002*\"year\" + 0.002*\"world\" + 0.002*\"american\"\n",
      "2020-08-01 21:26:44,853 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:44,857 :INFO :topic #2 (0.149): 0.003*\"also\" + 0.003*\"agave\" + 0.003*\"state\" + 0.002*\"first\" + 0.002*\"person\" + 0.002*\"many\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"world\"\n",
      "2020-08-01 21:26:44,860 :INFO :topic #1 (0.190): 0.003*\"also\" + 0.002*\"american\" + 0.002*\"football\" + 0.002*\"state\" + 0.002*\"first\" + 0.001*\"player\" + 0.001*\"acid\" + 0.001*\"new\" + 0.001*\"war\" + 0.001*\"city\"\n",
      "2020-08-01 21:26:44,863 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.001*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:44,867 :INFO :topic diff=0.140612, rho=0.122119\n",
      "2020-08-01 21:26:46,177 :INFO :-9.492 per-word bound, 720.3 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:46,178 :INFO :PROGRESS: pass 4, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:47,052 :INFO :topic #6 (0.080): 0.005*\"also\" + 0.003*\"state\" + 0.003*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.002*\"war\" + 0.002*\"year\" + 0.002*\"world\" + 0.002*\"american\"\n",
      "2020-08-01 21:26:47,055 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:47,059 :INFO :topic #2 (0.149): 0.004*\"agave\" + 0.003*\"also\" + 0.003*\"state\" + 0.003*\"first\" + 0.002*\"person\" + 0.002*\"many\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"world\"\n",
      "2020-08-01 21:26:47,063 :INFO :topic #1 (0.190): 0.003*\"also\" + 0.003*\"american\" + 0.002*\"football\" + 0.002*\"player\" + 0.002*\"acid\" + 0.002*\"first\" + 0.002*\"state\" + 0.002*\"new\" + 0.002*\"audi\" + 0.001*\"war\"\n",
      "2020-08-01 21:26:47,066 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:47,071 :INFO :topic diff=0.135530, rho=0.121218\n",
      "2020-08-01 21:26:48,467 :INFO :-9.397 per-word bound, 674.4 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:48,469 :INFO :PROGRESS: pass 5, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:49,290 :INFO :topic #6 (0.080): 0.005*\"also\" + 0.004*\"state\" + 0.003*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.002*\"year\" + 0.002*\"world\" + 0.002*\"language\"\n",
      "2020-08-01 21:26:49,294 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:49,297 :INFO :topic #2 (0.149): 0.004*\"agave\" + 0.004*\"also\" + 0.003*\"state\" + 0.003*\"first\" + 0.002*\"many\" + 0.002*\"person\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"\n",
      "2020-08-01 21:26:49,302 :INFO :topic #1 (0.190): 0.003*\"also\" + 0.003*\"american\" + 0.002*\"football\" + 0.002*\"acid\" + 0.002*\"player\" + 0.002*\"first\" + 0.002*\"audi\" + 0.002*\"state\" + 0.002*\"new\" + 0.002*\"play\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-01 21:26:49,305 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:49,310 :INFO :topic diff=0.134342, rho=0.120337\n",
      "2020-08-01 21:26:50,654 :INFO :-9.326 per-word bound, 641.8 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:50,656 :INFO :PROGRESS: pass 6, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:51,467 :INFO :topic #6 (0.080): 0.005*\"also\" + 0.004*\"state\" + 0.003*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"language\"\n",
      "2020-08-01 21:26:51,470 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:51,474 :INFO :topic #2 (0.149): 0.004*\"agave\" + 0.004*\"also\" + 0.003*\"state\" + 0.003*\"first\" + 0.002*\"many\" + 0.002*\"person\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"\n",
      "2020-08-01 21:26:51,477 :INFO :topic #1 (0.190): 0.003*\"also\" + 0.003*\"american\" + 0.003*\"football\" + 0.002*\"acid\" + 0.002*\"player\" + 0.002*\"audi\" + 0.002*\"first\" + 0.002*\"play\" + 0.002*\"ball\" + 0.002*\"new\"\n",
      "2020-08-01 21:26:51,480 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:51,484 :INFO :topic diff=0.135286, rho=0.119476\n",
      "2020-08-01 21:26:52,962 :INFO :-9.269 per-word bound, 616.8 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:52,963 :INFO :PROGRESS: pass 7, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:53,761 :INFO :topic #6 (0.080): 0.006*\"also\" + 0.004*\"state\" + 0.004*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"language\"\n",
      "2020-08-01 21:26:53,763 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.001*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:53,767 :INFO :topic #2 (0.149): 0.005*\"agave\" + 0.004*\"also\" + 0.003*\"state\" + 0.003*\"first\" + 0.002*\"many\" + 0.002*\"person\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"\n",
      "2020-08-01 21:26:53,771 :INFO :topic #1 (0.190): 0.003*\"american\" + 0.003*\"also\" + 0.003*\"football\" + 0.003*\"acid\" + 0.003*\"player\" + 0.002*\"audi\" + 0.002*\"first\" + 0.002*\"ball\" + 0.002*\"play\" + 0.002*\"team\"\n",
      "2020-08-01 21:26:53,774 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:53,777 :INFO :topic diff=0.137386, rho=0.118632\n",
      "2020-08-01 21:26:55,127 :INFO :-9.221 per-word bound, 596.7 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:55,129 :INFO :PROGRESS: pass 8, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:56,008 :INFO :topic #6 (0.080): 0.006*\"also\" + 0.004*\"state\" + 0.004*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"language\"\n",
      "2020-08-01 21:26:56,011 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.000*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:56,014 :INFO :topic #2 (0.149): 0.005*\"agave\" + 0.004*\"also\" + 0.004*\"state\" + 0.003*\"first\" + 0.002*\"many\" + 0.002*\"person\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"\n",
      "2020-08-01 21:26:56,017 :INFO :topic #1 (0.190): 0.004*\"american\" + 0.004*\"also\" + 0.003*\"football\" + 0.003*\"acid\" + 0.003*\"player\" + 0.003*\"audi\" + 0.002*\"ball\" + 0.002*\"play\" + 0.002*\"team\" + 0.002*\"first\"\n",
      "2020-08-01 21:26:56,021 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:56,024 :INFO :topic diff=0.140051, rho=0.117806\n",
      "2020-08-01 21:26:57,465 :INFO :-9.180 per-word bound, 580.1 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n",
      "2020-08-01 21:26:57,466 :INFO :PROGRESS: pass 9, dispatched chunk #0 = documents up to #111/111, outstanding queue size 1\n",
      "2020-08-01 21:26:58,253 :INFO :topic #6 (0.080): 0.006*\"also\" + 0.004*\"state\" + 0.004*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"language\"\n",
      "2020-08-01 21:26:58,256 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.000*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:58,260 :INFO :topic #2 (0.149): 0.005*\"agave\" + 0.004*\"also\" + 0.004*\"state\" + 0.003*\"first\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"person\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"\n",
      "2020-08-01 21:26:58,263 :INFO :topic #1 (0.190): 0.004*\"american\" + 0.004*\"also\" + 0.004*\"football\" + 0.003*\"acid\" + 0.003*\"player\" + 0.003*\"audi\" + 0.003*\"ball\" + 0.003*\"play\" + 0.003*\"team\" + 0.002*\"first\"\n",
      "2020-08-01 21:26:58,267 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:26:58,271 :INFO :topic diff=0.142885, rho=0.116997\n",
      "2020-08-01 21:26:59,585 :INFO :-9.145 per-word bound, 566.0 perplexity estimate based on a held-out corpus of 111 documents with 425825 words\n"
     ]
    }
   ],
   "source": [
    "lda_model=LdaMulticore(corpus,id2word=dict,random_state=100,num_topics=7,passes=10,batch=False,alpha='asymmetric',decay=0.5,offset=64,eta=None,eval_every=0,iterations=100,\n",
    "                   gamma_threshold=0.001,per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###lda_model=LdaMulticore(corpus,id2word=dict,random_state=100,num_topics=7,passes=10,chunksize=1000,batch=False,alpha='asymmetric',decay=0.5,offset=64,eta=None,eval_every=0,iterations=100,\n",
    "###                   gamma_threshold=0.001,per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-01 21:26:59,976 :INFO :saving LdaState object under lda_model.model.state, separately None\n",
      "2020-08-01 21:27:00,196 :INFO :saved lda_model.model.state\n",
      "2020-08-01 21:27:00,254 :INFO :saving LdaMulticore object under lda_model.model, separately ['expElogbeta', 'sstats']\n",
      "2020-08-01 21:27:00,255 :INFO :storing np array 'expElogbeta' to lda_model.model.expElogbeta.npy\n",
      "2020-08-01 21:27:00,475 :INFO :not storing attribute state\n",
      "2020-08-01 21:27:00,479 :INFO :not storing attribute dispatcher\n",
      "2020-08-01 21:27:00,483 :INFO :not storing attribute id2word\n",
      "2020-08-01 21:27:00,492 :INFO :saved lda_model.model\n"
     ]
    }
   ],
   "source": [
    "lda_model.save('lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-01 21:27:00,513 :INFO :topic #0 (0.262): 0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"\n",
      "2020-08-01 21:27:00,519 :INFO :topic #1 (0.190): 0.004*\"american\" + 0.004*\"also\" + 0.004*\"football\" + 0.003*\"acid\" + 0.003*\"player\" + 0.003*\"audi\" + 0.003*\"ball\" + 0.003*\"play\" + 0.003*\"team\" + 0.002*\"first\"\n",
      "2020-08-01 21:27:00,524 :INFO :topic #2 (0.149): 0.005*\"agave\" + 0.004*\"also\" + 0.004*\"state\" + 0.003*\"first\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"person\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"\n",
      "2020-08-01 21:27:00,528 :INFO :topic #3 (0.123): 0.006*\"american\" + 0.004*\"also\" + 0.004*\"first\" + 0.004*\"year\" + 0.003*\"state\" + 0.003*\"time\" + 0.003*\"day\" + 0.003*\"world\" + 0.002*\"many\" + 0.002*\"new\"\n",
      "2020-08-01 21:27:00,531 :INFO :topic #4 (0.104): 0.001*\"also\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"number\" + 0.000*\"world\" + 0.000*\"day\"\n",
      "2020-08-01 21:27:00,533 :INFO :topic #5 (0.091): 0.001*\"american\" + 0.001*\"also\" + 0.000*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"\n",
      "2020-08-01 21:27:00,536 :INFO :topic #6 (0.080): 0.006*\"also\" + 0.004*\"state\" + 0.004*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"language\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"also\" + 0.000*\"first\" + 0.000*\"time\" + 0.000*\"state\" + 0.000*\"person\" + 0.000*\"american\" + 0.000*\"year\" + 0.000*\"many\" + 0.000*\"new\" + 0.000*\"war\"'),\n",
       " (1,\n",
       "  '0.004*\"american\" + 0.004*\"also\" + 0.004*\"football\" + 0.003*\"acid\" + 0.003*\"player\" + 0.003*\"audi\" + 0.003*\"ball\" + 0.003*\"play\" + 0.003*\"team\" + 0.002*\"first\"'),\n",
       " (2,\n",
       "  '0.005*\"agave\" + 0.004*\"also\" + 0.004*\"state\" + 0.003*\"first\" + 0.003*\"many\" + 0.003*\"time\" + 0.003*\"person\" + 0.002*\"book\" + 0.002*\"year\" + 0.002*\"apollo\"'),\n",
       " (3,\n",
       "  '0.006*\"american\" + 0.004*\"also\" + 0.004*\"first\" + 0.004*\"year\" + 0.003*\"state\" + 0.003*\"time\" + 0.003*\"day\" + 0.003*\"world\" + 0.002*\"many\" + 0.002*\"new\"'),\n",
       " (4,\n",
       "  '0.001*\"also\" + 0.001*\"american\" + 0.000*\"state\" + 0.000*\"first\" + 0.000*\"many\" + 0.000*\"time\" + 0.000*\"year\" + 0.000*\"number\" + 0.000*\"world\" + 0.000*\"day\"'),\n",
       " (5,\n",
       "  '0.001*\"american\" + 0.001*\"also\" + 0.000*\"first\" + 0.000*\"year\" + 0.000*\"name\" + 0.000*\"albert\" + 0.000*\"world\" + 0.000*\"new\" + 0.000*\"however\" + 0.000*\"war\"'),\n",
       " (6,\n",
       "  '0.006*\"also\" + 0.004*\"state\" + 0.004*\"many\" + 0.003*\"person\" + 0.003*\"time\" + 0.003*\"first\" + 0.003*\"war\" + 0.003*\"world\" + 0.003*\"year\" + 0.002*\"language\"')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>LDA Observation</span>\n",
    "- Words viz. 'many', 'also' have occured across many topics , may be we can bring these words in teh STOP WORD list\n",
    "- LDAMulticore () supports multi processors , may be we can run LDAModel()\n",
    "\n",
    "# <span style='color:purple'>LDA Interpretation</span>\n",
    "- If we pass a list of words (document) to the model i.e. lda_model\n",
    "- Three things (a) Topics that document belongs to with % (b) Topic each word in that document belongs to (c) (b) and PHI value\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the LDA_MODEL against a set of corpus , say corpus [5:8]\n",
    "__Output produces the following:-__\n",
    "- Distribution of topic for each document, i.e. list of topics with its % for each document say \n",
    "- List of topic for each word in each document\n",
    "- PHI value : for each word in each document what is the % of each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document topics            : [(2, 0.97002685), (6, 0.029784068)]\n",
      "Word id, topic             : [(0, [2, 6]), (7, [2, 6]), (10, [2, 6])]\n",
      "Word id, topic & PHI val   : [(0, [(2, 2.8966966), (6, 0.10330065)]), (7, [(2, 0.97934216), (6, 0.020654295)]), (10, [(2, 0.96770024), (6, 0.032299228)])]\n",
      "word, topic                : [('ability', [2, 6]), ('absurdity', [2, 6])]\n",
      "word, PHI Value            : [('ability', [(2, 2.8966966), (6, 0.10330065)]), ('absurdity', [(2, 0.97934216), (6, 0.020654295)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(2, 0.9615397), (6, 0.03827158)]\n",
      "Word id, topic             : [(0, [2, 6]), (10, [2, 6]), (16, [2, 6])]\n",
      "Word id, topic & PHI val   : [(0, [(2, 5.7346497), (6, 0.26534584)]), (10, [(2, 2.8754706), (6, 0.12452758)]), (16, [(2, 0.9751281), (6, 0.024870958)])]\n",
      "word, topic                : [('ability', [2, 6]), ('academic', [2, 6])]\n",
      "word, PHI Value            : [('ability', [(2, 5.7346497), (6, 0.26534584)]), ('academic', [(2, 2.8754706), (6, 0.12452758)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(3, 0.5657904), (6, 0.43403763)]\n",
      "Word id, topic             : [(1, [3, 6]), (10, [6, 3]), (15, [3, 6])]\n",
      "Word id, topic & PHI val   : [(1, [(3, 0.5476864), (6, 0.45231247)]), (10, [(3, 2.5543056), (6, 3.4456894)]), (15, [(3, 1.2608366), (6, 0.73916155)])]\n",
      "word, topic                : [('able', [3, 6]), ('academic', [6, 3])]\n",
      "word, PHI Value            : [('able', [(3, 0.5476864), (6, 0.45231247)]), ('academic', [(3, 2.5543056), (6, 3.4456894)])]\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in lda_model[corpus[5:8]]:\n",
    "    print('Document topics            :',c[0])\n",
    "    print('Word id, topic             :',c[1][:3])\n",
    "    print('Word id, topic & PHI val   :',c[2][:3]) #[(word id,[(topic, phi value)])]\n",
    "    print('word, topic                :',[(dict[wd],topic) for wd,topic in c[1][:2]])\n",
    "    print('word, PHI Value            :',[(dict[wd],topic) for wd,topic in c[2][:2]])\n",
    "    print('--------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the LDA_MODEL against a set of corpus , say Atul Sood corpus ,which is last 11 records\n",
    "__Output produces the following:-__\n",
    "- Distribution of topic for each document, i.e. list of topics with its % for each document say \n",
    "- List of topic for each word in each document\n",
    "- PHI value : for each word in each document what is the % of each topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document topics            : [(0, 0.010456186), (1, 0.9684362)]\n",
      "Word id, topic             : [(215, [1]), (219, [1]), (984, [1])]\n",
      "Word id, topic & PHI val   : [(215, [(1, 0.99979466)]), (219, [(1, 0.99985415)]), (984, [(1, 0.99980354)])]\n",
      "word, topic                : [('certainly', [1]), ('change', [1])]\n",
      "word, PHI Value            : [('certainly', [(1, 0.99979466)]), ('change', [(1, 0.99985415)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(6, 0.97004974)]\n",
      "Word id, topic             : [(155, [6]), (227, [6]), (937, [6])]\n",
      "Word id, topic & PHI val   : [(155, [(6, 0.99985087)]), (227, [(6, 0.9998393)]), (937, [(6, 0.99975485)])]\n",
      "word, topic                : [('best', [6]), ('chief', [6])]\n",
      "word, PHI Value            : [('best', [(6, 0.99985087)]), ('chief', [(6, 0.9998393)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(0, 0.024208438), (1, 0.017531896), (2, 0.013670664), (3, 0.011184189), (6, 0.9156658)]\n",
      "Word id, topic             : [(741, [6]), (843, [6]), (1191, [6])]\n",
      "Word id, topic & PHI val   : [(741, [(6, 0.99971545)]), (843, [(6, 0.9997334)]), (1191, [(6, 0.9994746)])]\n",
      "word, topic                : [('impossible', [6]), ('language', [6])]\n",
      "word, PHI Value            : [('impossible', [(6, 0.99971545)]), ('language', [(6, 0.9997334)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(2, 0.6819638), (6, 0.30176613)]\n",
      "Word id, topic             : [(54, [2, 6]), (193, [2, 6]), (355, [2, 6])]\n",
      "Word id, topic & PHI val   : [(54, [(2, 0.63104856), (6, 0.3688321)]), (193, [(2, 0.71513253), (6, 0.2847876)]), (355, [(2, 0.69042856), (6, 0.30950817)])]\n",
      "word, topic                : [('also', [2, 6]), ('call', [2, 6])]\n",
      "word, PHI Value            : [('also', [(2, 0.63104856), (6, 0.3688321)]), ('call', [(2, 0.71513253), (6, 0.2847876)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(5, 0.7701659), (6, 0.20840237)]\n",
      "Word id, topic             : [(445, [6, 5]), (517, [6, 5]), (523, [6, 5])]\n",
      "Word id, topic & PHI val   : [(445, [(5, 0.48083052), (6, 0.5186459)]), (517, [(5, 0.3590935), (6, 0.64063585)]), (523, [(5, 0.38143426), (6, 0.6180269)])]\n",
      "word, topic                : [('earth', [6, 5]), ('ever', [6, 5])]\n",
      "word, PHI Value            : [('earth', [(5, 0.48083052), (6, 0.5186459)]), ('ever', [(5, 0.3590935), (6, 0.64063585)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(0, 0.7036049), (6, 0.2763647)]\n",
      "Word id, topic             : [(280, [6, 0]), (402, [6, 0]), (704, [6, 0])]\n",
      "Word id, topic & PHI val   : [(280, [(0, 0.4741533), (6, 0.52580667)]), (402, [(0, 0.31156653), (6, 0.68812406)]), (704, [(0, 0.23741561), (6, 0.7623822)])]\n",
      "word, topic                : [('communism', [6, 0]), ('different', [6, 0])]\n",
      "word, PHI Value            : [('communism', [(0, 0.4741533), (6, 0.52580667)]), ('different', [(0, 0.31156653), (6, 0.68812406)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(0, 0.021128532), (1, 0.014726319), (2, 0.01152408), (6, 0.9281495)]\n",
      "Word id, topic             : [(215, [6]), (581, [6]), (1514, [6])]\n",
      "Word id, topic & PHI val   : [(215, [(6, 0.9994694)]), (581, [(6, 0.9997225)]), (1514, [(6, 0.9997127)])]\n",
      "word, topic                : [('certainly', [6]), ('fiction', [6])]\n",
      "word, PHI Value            : [('certainly', [(6, 0.9994694)]), ('fiction', [(6, 0.9997225)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(3, 0.97727436)]\n",
      "Word id, topic             : [(275, [3]), (312, [3]), (485, [3])]\n",
      "Word id, topic & PHI val   : [(275, [(3, 0.99985856)]), (312, [(3, 0.9998702)]), (485, [(3, 1.999889)])]\n",
      "word, topic                : [('common', [3]), ('consequently', [3])]\n",
      "word, PHI Value            : [('common', [(3, 0.99985856)]), ('consequently', [(3, 0.9998702)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(0, 0.019003099), (1, 0.013731374), (2, 0.010711926), (4, 0.93555444)]\n",
      "Word id, topic             : [(1521, [4]), (2248, [4]), (9981, [4])]\n",
      "Word id, topic & PHI val   : [(1521, [(4, 0.99746263)]), (2248, [(4, 0.9983352)]), (9981, [(4, 0.9997026)])]\n",
      "word, topic                : [('today', [4]), ('india', [4])]\n",
      "word, PHI Value            : [('today', [(4, 0.99746263)]), ('india', [(4, 0.9983352)])]\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Document topics            : [(0, 0.017906614), (1, 0.012862574), (6, 0.9380216)]\n",
      "Word id, topic             : [(198, [6]), (656, [6]), (895, [6])]\n",
      "Word id, topic & PHI val   : [(198, [(6, 0.9996247)]), (656, [(6, 0.999784)]), (895, [(6, 0.9997166)])]\n",
      "word, topic                : [('capital', [6]), ('god', [6])]\n",
      "word, PHI Value            : [('capital', [(6, 0.9996247)]), ('god', [(6, 0.999784)])]\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in lda_model[corpus[101:111]]:\n",
    "    print('Document topics            :',c[0])\n",
    "    print('Word id, topic             :',c[1][:3])\n",
    "    print('Word id, topic & PHI val   :',c[2][:3]) #[(word id,[(topic, phi value)])]\n",
    "    print('word, topic                :',[(dict[wd],topic) for wd,topic in c[1][:2]])\n",
    "    print('word, PHI Value            :',[(dict[wd],topic) for wd,topic in c[2][:2]])\n",
    "    print('--------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[(dict[i],count) for i,count in corpus[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>LSI : Latent Semantic Information</span>\n",
    "- Similar to LDA except using LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 11:43:22,509 :INFO :using serial LSI version on this node\n",
      "2020-08-02 11:43:22,513 :INFO :updating model with new documents\n",
      "2020-08-02 11:43:22,648 :INFO :preparing a new chunk of documents\n",
      "2020-08-02 11:43:22,803 :INFO :using 100 extra samples and 2 power iterations\n",
      "2020-08-02 11:43:22,805 :INFO :1st phase: constructing (40130, 107) action matrix\n",
      "2020-08-02 11:43:22,936 :INFO :orthonormalizing (40130, 107) action matrix\n",
      "2020-08-02 11:43:26,763 :INFO :2nd phase: running dense svd on (107, 111) matrix\n",
      "2020-08-02 11:43:27,164 :INFO :computing the final decomposition\n",
      "2020-08-02 11:43:27,193 :INFO :keeping 7 factors (discarding 62.873% of energy spectrum)\n",
      "2020-08-02 11:43:27,410 :INFO :processed documents up to #111\n",
      "2020-08-02 11:43:27,616 :INFO :topic #0(973.796): -0.262*\"also\" + -0.197*\"state\" + -0.197*\"american\" + -0.178*\"first\" + -0.151*\"many\" + -0.149*\"time\" + -0.147*\"year\" + -0.130*\"person\" + -0.130*\"world\" + -0.124*\"war\"\n",
      "2020-08-02 11:43:27,620 :INFO :topic #1(572.318): -0.937*\"agave\" + -0.164*\"asia\" + -0.100*\"aruba\" + -0.063*\"plant\" + -0.053*\"var\" + -0.052*\"state\" + -0.045*\"east\" + -0.044*\"congress\" + 0.042*\"first\" + -0.041*\"maguey\"\n",
      "2020-08-02 11:43:27,624 :INFO :topic #2(401.785): -0.507*\"american\" + -0.180*\"football\" + -0.179*\"player\" + -0.168*\"war\" + -0.150*\"british\" + 0.140*\"also\" + -0.114*\"ball\" + -0.110*\"day\" + 0.107*\"atheism\" + 0.106*\"god\"\n",
      "2020-08-02 11:43:27,628 :INFO :topic #3(334.148): 0.362*\"apollo\" + -0.248*\"lincoln\" + -0.211*\"state\" + 0.172*\"player\" + 0.151*\"football\" + -0.127*\"union\" + 0.125*\"ball\" + -0.124*\"government\" + 0.116*\"moon\" + -0.116*\"jews\"\n",
      "2020-08-02 11:43:27,631 :INFO :topic #4(322.190): 0.363*\"atheism\" + 0.334*\"god\" + 0.329*\"lincoln\" + 0.230*\"apollo\" + 0.215*\"atheist\" + 0.143*\"abraham\" + -0.136*\"island\" + 0.132*\"aristotle\" + -0.124*\"aluminium\" + 0.119*\"belief\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model = LsiModel(corpus,num_topics=7,id2word=dict,decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 11:43:37,546 :INFO :topic #0(973.796): -0.262*\"also\" + -0.197*\"state\" + -0.197*\"american\" + -0.178*\"first\" + -0.151*\"many\" + -0.149*\"time\" + -0.147*\"year\" + -0.130*\"person\" + -0.130*\"world\" + -0.124*\"war\"\n",
      "2020-08-02 11:43:37,551 :INFO :topic #1(572.318): -0.937*\"agave\" + -0.164*\"asia\" + -0.100*\"aruba\" + -0.063*\"plant\" + -0.053*\"var\" + -0.052*\"state\" + -0.045*\"east\" + -0.044*\"congress\" + 0.042*\"first\" + -0.041*\"maguey\"\n",
      "2020-08-02 11:43:37,555 :INFO :topic #2(401.785): -0.507*\"american\" + -0.180*\"football\" + -0.179*\"player\" + -0.168*\"war\" + -0.150*\"british\" + 0.140*\"also\" + -0.114*\"ball\" + -0.110*\"day\" + 0.107*\"atheism\" + 0.106*\"god\"\n",
      "2020-08-02 11:43:37,559 :INFO :topic #3(334.148): 0.362*\"apollo\" + -0.248*\"lincoln\" + -0.211*\"state\" + 0.172*\"player\" + 0.151*\"football\" + -0.127*\"union\" + 0.125*\"ball\" + -0.124*\"government\" + 0.116*\"moon\" + -0.116*\"jews\"\n",
      "2020-08-02 11:43:37,562 :INFO :topic #4(322.190): 0.363*\"atheism\" + 0.334*\"god\" + 0.329*\"lincoln\" + 0.230*\"apollo\" + 0.215*\"atheist\" + 0.143*\"abraham\" + -0.136*\"island\" + 0.132*\"aristotle\" + -0.124*\"aluminium\" + 0.119*\"belief\"\n",
      "2020-08-02 11:43:37,566 :INFO :topic #5(315.315): -0.360*\"apollo\" + 0.344*\"atheism\" + -0.326*\"lincoln\" + 0.226*\"god\" + 0.205*\"atheist\" + 0.139*\"american\" + -0.130*\"lunar\" + 0.127*\"football\" + -0.125*\"moon\" + 0.114*\"belief\"\n",
      "2020-08-02 11:43:37,569 :INFO :topic #6(312.092): -0.313*\"lincoln\" + 0.226*\"apollo\" + -0.166*\"football\" + -0.163*\"war\" + 0.162*\"god\" + 0.153*\"australia\" + -0.148*\"play\" + -0.146*\"ball\" + 0.122*\"atheism\" + -0.122*\"line\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '-0.262*\"also\" + -0.197*\"state\" + -0.197*\"american\" + -0.178*\"first\" + -0.151*\"many\" + -0.149*\"time\" + -0.147*\"year\" + -0.130*\"person\" + -0.130*\"world\" + -0.124*\"war\"'), (1, '-0.937*\"agave\" + -0.164*\"asia\" + -0.100*\"aruba\" + -0.063*\"plant\" + -0.053*\"var\" + -0.052*\"state\" + -0.045*\"east\" + -0.044*\"congress\" + 0.042*\"first\" + -0.041*\"maguey\"'), (2, '-0.507*\"american\" + -0.180*\"football\" + -0.179*\"player\" + -0.168*\"war\" + -0.150*\"british\" + 0.140*\"also\" + -0.114*\"ball\" + -0.110*\"day\" + 0.107*\"atheism\" + 0.106*\"god\"'), (3, '0.362*\"apollo\" + -0.248*\"lincoln\" + -0.211*\"state\" + 0.172*\"player\" + 0.151*\"football\" + -0.127*\"union\" + 0.125*\"ball\" + -0.124*\"government\" + 0.116*\"moon\" + -0.116*\"jews\"'), (4, '0.363*\"atheism\" + 0.334*\"god\" + 0.329*\"lincoln\" + 0.230*\"apollo\" + 0.215*\"atheist\" + 0.143*\"abraham\" + -0.136*\"island\" + 0.132*\"aristotle\" + -0.124*\"aluminium\" + 0.119*\"belief\"'), (5, '-0.360*\"apollo\" + 0.344*\"atheism\" + -0.326*\"lincoln\" + 0.226*\"god\" + 0.205*\"atheist\" + 0.139*\"american\" + -0.130*\"lunar\" + 0.127*\"football\" + -0.125*\"moon\" + 0.114*\"belief\"'), (6, '-0.313*\"lincoln\" + 0.226*\"apollo\" + -0.166*\"football\" + -0.163*\"war\" + 0.162*\"god\" + 0.153*\"australia\" + -0.148*\"play\" + -0.146*\"ball\" + 0.122*\"atheism\" + -0.122*\"line\"')]\n"
     ]
    }
   ],
   "source": [
    "print(lsi_model.print_topics(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:blue'>Word2Vec : Train using W2V using gensim</span>\n",
    "- USe prebuilt model like word2vec, fasttext, GloVe, ConceptNet : Built using large corpuses using wikipedia, googleNews etc.\n",
    "- FOr specialized document - viz Technical document, TRAIN model\n",
    "- Below TRAIN word embedding using gensim and self corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download dataset\n",
    "dataset=api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       " 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       " 'file_name': 'text8.gz',\n",
       " 'file_size': 33182058,\n",
       " 'license': 'not found',\n",
       " 'num_records': 1701,\n",
       " 'parts': 1,\n",
       " 'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       " 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       " 'record_format': 'list of str (tokens)'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.info('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df7=pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df7.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =[d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into two part \n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10000)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 10000)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_part2[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5207,)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_part2[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:01:01,292 :INFO :collecting all words and their counts\n",
      "2020-08-02 12:01:01,553 :INFO :PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-08-02 12:01:04,755 :INFO :collected 189074 word types from a corpus of 10000000 raw words and 1000 sentences\n",
      "2020-08-02 12:01:04,786 :INFO :Loading a fresh vocabulary\n",
      "2020-08-02 12:01:25,614 :INFO :effective_min_count=0 retains 189074 unique words (100% of original 189074, drops 0)\n",
      "2020-08-02 12:01:25,615 :INFO :effective_min_count=0 leaves 10000000 word corpus (100% of original 10000000, drops 0)\n",
      "2020-08-02 12:01:26,395 :INFO :deleting the raw counts dictionary of 189074 items\n",
      "2020-08-02 12:01:26,401 :INFO :sample=0.001 downsamples 38 most-common words\n",
      "2020-08-02 12:01:26,403 :INFO :downsampling leaves estimated 7563517 word corpus (75.6% of prior 10000000)\n",
      "2020-08-02 12:01:27,162 :INFO :estimated required memory for 189074 words and 100 dimensions: 245796200 bytes\n",
      "2020-08-02 12:01:27,164 :INFO :resetting layer weights\n",
      "2020-08-02 12:01:29,648 :INFO :training model with 4 workers on 189074 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-08-02 12:01:30,751 :INFO :EPOCH 1 - PROGRESS: at 9.80% examples, 729274 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:31,769 :INFO :EPOCH 1 - PROGRESS: at 21.50% examples, 794862 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:01:32,774 :INFO :EPOCH 1 - PROGRESS: at 32.70% examples, 809531 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:33,793 :INFO :EPOCH 1 - PROGRESS: at 44.70% examples, 830435 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:01:34,793 :INFO :EPOCH 1 - PROGRESS: at 56.20% examples, 839685 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:35,794 :INFO :EPOCH 1 - PROGRESS: at 68.10% examples, 849679 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:36,808 :INFO :EPOCH 1 - PROGRESS: at 79.80% examples, 853205 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:37,813 :INFO :EPOCH 1 - PROGRESS: at 91.70% examples, 859235 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:38,505 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:01:38,513 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:01:38,528 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:01:38,547 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:01:38,548 :INFO :EPOCH - 1 : training on 10000000 raw words (7563775 effective words) took 8.8s, 858705 effective words/s\n",
      "2020-08-02 12:01:39,695 :INFO :EPOCH 2 - PROGRESS: at 12.10% examples, 905237 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:40,702 :INFO :EPOCH 2 - PROGRESS: at 24.10% examples, 901670 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:41,707 :INFO :EPOCH 2 - PROGRESS: at 35.90% examples, 895309 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:42,713 :INFO :EPOCH 2 - PROGRESS: at 47.60% examples, 892024 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:43,716 :INFO :EPOCH 2 - PROGRESS: at 59.40% examples, 893154 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:01:44,723 :INFO :EPOCH 2 - PROGRESS: at 71.20% examples, 892104 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:45,735 :INFO :EPOCH 2 - PROGRESS: at 82.90% examples, 889816 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:01:46,747 :INFO :EPOCH 2 - PROGRESS: at 94.50% examples, 887645 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:01:47,195 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:01:47,204 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:01:47,217 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:01:47,225 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:01:47,227 :INFO :EPOCH - 2 : training on 10000000 raw words (7562854 effective words) took 8.5s, 886264 effective words/s\n",
      "2020-08-02 12:01:48,236 :INFO :EPOCH 3 - PROGRESS: at 11.30% examples, 842866 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:01:49,241 :INFO :EPOCH 3 - PROGRESS: at 23.00% examples, 859245 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:01:50,243 :INFO :EPOCH 3 - PROGRESS: at 34.90% examples, 870161 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:01:51,249 :INFO :EPOCH 3 - PROGRESS: at 46.70% examples, 875044 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:52,250 :INFO :EPOCH 3 - PROGRESS: at 58.20% examples, 875726 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:53,256 :INFO :EPOCH 3 - PROGRESS: at 68.70% examples, 861176 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:01:54,266 :INFO :EPOCH 3 - PROGRESS: at 78.80% examples, 846329 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:55,271 :INFO :EPOCH 3 - PROGRESS: at 90.40% examples, 850330 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:56,069 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:01:56,072 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:01:56,087 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:01:56,098 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:01:56,099 :INFO :EPOCH - 3 : training on 10000000 raw words (7561820 effective words) took 8.9s, 852755 effective words/s\n",
      "2020-08-02 12:01:57,140 :INFO :EPOCH 4 - PROGRESS: at 11.70% examples, 877244 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:58,146 :INFO :EPOCH 4 - PROGRESS: at 23.50% examples, 879665 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:01:59,148 :INFO :EPOCH 4 - PROGRESS: at 35.50% examples, 886368 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:00,150 :INFO :EPOCH 4 - PROGRESS: at 47.10% examples, 884611 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:01,152 :INFO :EPOCH 4 - PROGRESS: at 58.70% examples, 884342 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:02:02,166 :INFO :EPOCH 4 - PROGRESS: at 70.40% examples, 882201 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:03,167 :INFO :EPOCH 4 - PROGRESS: at 82.30% examples, 885000 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:04,170 :INFO :EPOCH 4 - PROGRESS: at 93.80% examples, 883502 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:04,676 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:02:04,686 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:02:04,695 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:02:04,704 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:02:04,706 :INFO :EPOCH - 4 : training on 10000000 raw words (7562104 effective words) took 8.6s, 882729 effective words/s\n",
      "2020-08-02 12:02:05,718 :INFO :EPOCH 5 - PROGRESS: at 11.60% examples, 863898 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:02:06,721 :INFO :EPOCH 5 - PROGRESS: at 23.50% examples, 878121 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:07,724 :INFO :EPOCH 5 - PROGRESS: at 35.30% examples, 880090 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:08,726 :INFO :EPOCH 5 - PROGRESS: at 46.90% examples, 879920 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:02:09,736 :INFO :EPOCH 5 - PROGRESS: at 58.70% examples, 882260 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:10,748 :INFO :EPOCH 5 - PROGRESS: at 70.50% examples, 882052 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:02:11,752 :INFO :EPOCH 5 - PROGRESS: at 82.40% examples, 884640 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:02:12,762 :INFO :EPOCH 5 - PROGRESS: at 94.20% examples, 885109 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:02:13,228 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:02:13,237 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:02:13,250 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:02:13,266 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:02:13,268 :INFO :EPOCH - 5 : training on 10000000 raw words (7562981 effective words) took 8.6s, 883934 effective words/s\n",
      "2020-08-02 12:02:13,270 :INFO :training on a 50000000 raw words (37813534 effective words) took 43.6s, 866900 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#Train W2V with default vector size = 100\n",
    "model=Word2Vec(data_part1,min_count=0,workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.5262312 ,  0.409382  , -1.4726707 ,  0.39713287,  0.24458115,\n",
       "        0.6964405 , -0.6542572 , -0.28446203, -0.4563818 ,  0.12563562,\n",
       "       -0.27262893, -0.2628879 , -0.58937395,  1.076456  , -0.14841628,\n",
       "        0.9316121 , -1.0665156 , -0.7353373 ,  0.30655625, -1.6143473 ,\n",
       "        0.42842793, -0.15731955,  1.9499857 ,  0.5797155 ,  1.3734457 ,\n",
       "       -1.1049469 , -1.3376522 ,  0.6532739 , -0.69266707, -0.92454493,\n",
       "        0.19630088, -0.28022358,  0.7154724 ,  0.9280978 ,  0.19606432,\n",
       "        0.12112555, -0.05990876,  0.7903815 , -0.45332834,  0.12762949,\n",
       "       -0.14184141, -0.00800085,  0.2833234 ,  0.2683099 ,  0.45947027,\n",
       "        0.07656071,  0.48083988, -0.66036373,  0.62744874,  0.8435816 ,\n",
       "       -0.49784535,  1.6581378 , -0.5192989 , -0.4286661 ,  0.29243273,\n",
       "       -1.150825  ,  0.42950436,  0.0577182 ,  0.32688102, -0.21003073,\n",
       "       -0.89452416,  0.23006344, -0.42895955, -0.82256824, -0.50821704,\n",
       "        0.6943391 , -0.28252715,  0.3059836 ,  0.24493313, -0.08397363,\n",
       "        0.12399185,  0.64353937, -0.6023594 ,  0.5673604 , -0.32173017,\n",
       "        0.25618225, -0.03675764,  0.12108371,  0.98217916,  0.31859165,\n",
       "        0.2424747 , -0.59549326,  0.9193725 ,  0.11875612, -0.5387353 ,\n",
       "        0.38154054, -0.95472634,  0.46186993,  0.13221623,  0.5287064 ,\n",
       "       -0.78199023, -0.522324  ,  0.09927048,  1.0705957 ,  0.42500845,\n",
       "       -0.5342291 ,  0.27682352, -0.04776321, -1.2030537 , -0.6953918 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-08-02 12:07:30,591 :INFO :precomputing L2-norms of word weight vectors\n",
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('topic', 0.6691651344299316),\n",
       " ('question', 0.631722629070282),\n",
       " ('issue', 0.6240805983543396),\n",
       " ('debate', 0.6190211772918701),\n",
       " ('matter', 0.5940889120101929),\n",
       " ('contrary', 0.5872217416763306),\n",
       " ('opinion', 0.5860634446144104),\n",
       " ('interpretation', 0.5841245055198669),\n",
       " ('moral', 0.5747615098953247),\n",
       " ('irrelevant', 0.5735539197921753)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:07:41,307 :INFO :saving Word2Vec object under newmodel, separately None\n",
      "2020-08-02 12:07:41,341 :INFO :storing np array 'vectors' to newmodel.wv.vectors.npy\n",
      "2020-08-02 12:07:42,411 :INFO :not storing attribute vectors_norm\n",
      "2020-08-02 12:07:42,447 :INFO :storing np array 'syn1neg' to newmodel.trainables.syn1neg.npy\n",
      "2020-08-02 12:07:43,787 :INFO :not storing attribute cum_table\n",
      "2020-08-02 12:07:44,508 :INFO :saved newmodel\n"
     ]
    }
   ],
   "source": [
    "model.save('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:11:59,637 :INFO :loading Word2Vec object from newmodel\n",
      "2020-08-02 12:12:00,181 :INFO :loading wv recursively from newmodel.wv.* with mmap=None\n",
      "2020-08-02 12:12:00,182 :INFO :loading vectors from newmodel.wv.vectors.npy with mmap=None\n",
      "2020-08-02 12:12:00,250 :INFO :setting ignored attribute vectors_norm to None\n",
      "2020-08-02 12:12:00,251 :INFO :loading vocabulary recursively from newmodel.vocabulary.* with mmap=None\n",
      "2020-08-02 12:12:00,253 :INFO :loading trainables recursively from newmodel.trainables.* with mmap=None\n",
      "2020-08-02 12:12:00,255 :INFO :loading syn1neg from newmodel.trainables.syn1neg.npy with mmap=None\n",
      "2020-08-02 12:12:00,323 :INFO :setting ignored attribute cum_table to None\n",
      "2020-08-02 12:12:00,325 :INFO :loaded newmodel\n"
     ]
    }
   ],
   "source": [
    "model=Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus1=model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189074"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style='color:red'>Update existing Word2Vec Model with new data set</span>\n",
    "__Steps involved :-__\n",
    "- update dictionary by calling build_vocabulary\n",
    "- Train the model with sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:20:33,233 :INFO :collecting all words and their counts\n",
      "2020-08-02 12:20:33,236 :INFO :PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-08-02 12:20:35,013 :INFO :collected 153347 word types from a corpus of 7005207 raw words and 701 sentences\n",
      "2020-08-02 12:20:35,014 :INFO :Updating model with new vocabulary\n",
      "2020-08-02 12:20:36,029 :INFO :New added 153347 unique words (50% of original 306694) and increased the count of 153347 pre-existing words (50% of original 306694)\n",
      "2020-08-02 12:20:37,270 :INFO :deleting the raw counts dictionary of 153347 items\n",
      "2020-08-02 12:20:37,275 :INFO :sample=0.001 downsamples 72 most-common words\n",
      "2020-08-02 12:20:37,277 :INFO :downsampling leaves estimated 10509051 word corpus (150.0% of prior 7005207)\n",
      "2020-08-02 12:20:37,881 :INFO :estimated required memory for 306694 words and 100 dimensions: 398702200 bytes\n",
      "2020-08-02 12:20:37,882 :INFO :updating layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(data_part2,update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253854"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-08-02 12:35:54,125 :WARNING :Effective 'alpha' higher than previous training cycles\n",
      "2020-08-02 12:35:54,130 :INFO :training model with 4 workers on 253854 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-08-02 12:35:55,138 :INFO :EPOCH 1 - PROGRESS: at 15.26% examples, 805367 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:35:56,142 :INFO :EPOCH 1 - PROGRESS: at 32.24% examples, 850520 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:35:57,156 :INFO :EPOCH 1 - PROGRESS: at 48.36% examples, 840897 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:35:58,162 :INFO :EPOCH 1 - PROGRESS: at 65.34% examples, 853114 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:35:59,167 :INFO :EPOCH 1 - PROGRESS: at 81.74% examples, 854649 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:00,169 :INFO :EPOCH 1 - PROGRESS: at 98.15% examples, 855409 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:00,265 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:36:00,273 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:36:00,275 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:36:00,293 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:36:00,295 :INFO :EPOCH - 1 : training on 7005207 raw words (5254658 effective words) took 6.2s, 853520 effective words/s\n",
      "2020-08-02 12:36:01,306 :INFO :EPOCH 2 - PROGRESS: at 15.69% examples, 824485 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:02,308 :INFO :EPOCH 2 - PROGRESS: at 32.52% examples, 856517 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:36:03,318 :INFO :EPOCH 2 - PROGRESS: at 49.50% examples, 860419 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:04,322 :INFO :EPOCH 2 - PROGRESS: at 66.33% examples, 866548 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:36:05,333 :INFO :EPOCH 2 - PROGRESS: at 82.88% examples, 865889 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:06,335 :INFO :EPOCH 2 - PROGRESS: at 99.14% examples, 863575 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 12:36:06,366 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:36:06,368 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:36:06,371 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:36:06,381 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:36:06,382 :INFO :EPOCH - 2 : training on 7005207 raw words (5253755 effective words) took 6.1s, 863758 effective words/s\n",
      "2020-08-02 12:36:07,391 :INFO :EPOCH 3 - PROGRESS: at 15.55% examples, 819270 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:08,392 :INFO :EPOCH 3 - PROGRESS: at 31.81% examples, 839141 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:36:09,396 :INFO :EPOCH 3 - PROGRESS: at 48.79% examples, 851289 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:10,410 :INFO :EPOCH 3 - PROGRESS: at 65.62% examples, 857435 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:11,411 :INFO :EPOCH 3 - PROGRESS: at 81.74% examples, 855678 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:12,417 :INFO :EPOCH 3 - PROGRESS: at 98.43% examples, 858289 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:12,494 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:36:12,506 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:36:12,509 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:36:12,521 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:36:12,523 :INFO :EPOCH - 3 : training on 7005207 raw words (5255333 effective words) took 6.1s, 856677 effective words/s\n",
      "2020-08-02 12:36:13,530 :INFO :EPOCH 4 - PROGRESS: at 15.26% examples, 803726 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:36:14,532 :INFO :EPOCH 4 - PROGRESS: at 31.53% examples, 831231 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:15,536 :INFO :EPOCH 4 - PROGRESS: at 46.79% examples, 816419 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-02 12:36:16,548 :INFO :EPOCH 4 - PROGRESS: at 63.34% examples, 827500 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:17,554 :INFO :EPOCH 4 - PROGRESS: at 80.17% examples, 838782 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:18,556 :INFO :EPOCH 4 - PROGRESS: at 96.58% examples, 842050 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:18,811 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:36:18,821 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:36:18,823 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:36:18,829 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:36:18,831 :INFO :EPOCH - 4 : training on 7005207 raw words (5254650 effective words) took 6.3s, 833657 effective words/s\n",
      "2020-08-02 12:36:19,838 :INFO :EPOCH 5 - PROGRESS: at 14.98% examples, 788205 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:20,850 :INFO :EPOCH 5 - PROGRESS: at 31.38% examples, 823998 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:21,851 :INFO :EPOCH 5 - PROGRESS: at 48.22% examples, 839459 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 12:36:22,853 :INFO :EPOCH 5 - PROGRESS: at 65.05% examples, 850796 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:23,857 :INFO :EPOCH 5 - PROGRESS: at 81.03% examples, 848580 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-02 12:36:24,867 :INFO :EPOCH 5 - PROGRESS: at 95.72% examples, 834120 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-02 12:36:25,131 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:36:25,151 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:36:25,161 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:36:25,166 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:36:25,168 :INFO :EPOCH - 5 : training on 7005207 raw words (5254934 effective words) took 6.3s, 829877 effective words/s\n",
      "2020-08-02 12:36:25,170 :INFO :training on a 35026035 raw words (26273330 effective words) took 31.0s, 846498 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26273330, 35026035)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data_part2,total_examples=model.corpus_count,epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-08-02 12:37:02,070 :INFO :precomputing L2-norms of word weight vectors\n",
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('topic', 0.6607025265693665),\n",
       " ('matter', 0.6202406883239746),\n",
       " ('question', 0.5902384519577026),\n",
       " ('nature', 0.5512784719467163),\n",
       " ('scope', 0.5502288937568665),\n",
       " ('discussion', 0.5417620539665222),\n",
       " ('debate', 0.5377275347709656),\n",
       " ('perception', 0.5320144891738892),\n",
       " ('validity', 0.526340126991272),\n",
       " ('intent', 0.5252469182014465)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Sood's WA text msgs\n",
    "#tokenlized_wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:37:23,130 :INFO :collecting all words and their counts\n",
      "2020-08-02 12:37:23,133 :INFO :PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-08-02 12:37:23,191 :INFO :collected 340 word types from a corpus of 637 raw words and 11 sentences\n",
      "2020-08-02 12:37:23,193 :INFO :Updating model with new vocabulary\n",
      "2020-08-02 12:37:23,196 :INFO :New added 340 unique words (50% of original 680) and increased the count of 340 pre-existing words (50% of original 680)\n",
      "2020-08-02 12:37:23,227 :INFO :deleting the raw counts dictionary of 340 items\n",
      "2020-08-02 12:37:23,229 :INFO :sample=0.001 downsamples 206 most-common words\n",
      "2020-08-02 12:37:23,231 :INFO :downsampling leaves estimated 905 word corpus (142.2% of prior 637)\n",
      "2020-08-02 12:37:23,787 :INFO :estimated required memory for 680 words and 100 dimensions: 884000 bytes\n",
      "2020-08-02 12:37:23,788 :INFO :updating layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(tokenlized_wa,update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253885"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-08-02 12:37:55,756 :WARNING :Effective 'alpha' higher than previous training cycles\n",
      "2020-08-02 12:37:55,759 :INFO :training model with 4 workers on 253885 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-08-02 12:37:55,766 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:37:55,768 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:37:55,771 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:37:55,773 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:37:55,775 :INFO :EPOCH - 1 : training on 637 raw words (460 effective words) took 0.0s, 43110 effective words/s\n",
      "2020-08-02 12:37:55,784 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:37:55,786 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:37:55,787 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:37:55,832 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:37:55,834 :INFO :EPOCH - 2 : training on 637 raw words (443 effective words) took 0.1s, 8603 effective words/s\n",
      "2020-08-02 12:37:55,842 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:37:55,844 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:37:55,846 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:37:55,848 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:37:55,851 :INFO :EPOCH - 3 : training on 637 raw words (457 effective words) took 0.0s, 41895 effective words/s\n",
      "2020-08-02 12:37:55,858 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:37:55,860 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:37:55,862 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:37:55,864 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:37:55,866 :INFO :EPOCH - 4 : training on 637 raw words (466 effective words) took 0.0s, 47766 effective words/s\n",
      "2020-08-02 12:37:55,873 :INFO :worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-02 12:37:55,875 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 12:37:55,877 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 12:37:55,879 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 12:37:55,881 :INFO :EPOCH - 5 : training on 637 raw words (454 effective words) took 0.0s, 48794 effective words/s\n",
      "2020-08-02 12:37:55,883 :INFO :training on a 3185 raw words (2280 effective words) took 0.1s, 18719 effective words/s\n",
      "2020-08-02 12:37:55,885 :WARNING :under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2280, 3185)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(tokenlized_wa,total_examples=model.corpus_count,epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.train(data_part2,total_examples=model.corpus_count,epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2020-08-02 12:38:30,647 :INFO :precomputing L2-norms of word weight vectors\n",
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('topic', 0.6607025265693665),\n",
       " ('matter', 0.6202406883239746),\n",
       " ('question', 0.5902384519577026),\n",
       " ('nature', 0.5512784719467163),\n",
       " ('scope', 0.5502288937568665),\n",
       " ('discussion', 0.5417620539665222),\n",
       " ('debate', 0.5377275347709656),\n",
       " ('perception', 0.5320144891738892),\n",
       " ('validity', 0.526340126991272),\n",
       " ('intent', 0.5252469182014465)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('motilal', 0.8182438015937805),\n",
       " ('rajiv', 0.7659848928451538),\n",
       " ('sardar', 0.74310302734375),\n",
       " ('priyanka', 0.7360767126083374),\n",
       " ('zulfikar', 0.730406641960144),\n",
       " ('karmal', 0.7269328236579895),\n",
       " ('sanjay', 0.720064640045166),\n",
       " ('varun', 0.7084603905677795),\n",
       " ('mohandas', 0.7075860500335693),\n",
       " ('sonia', 0.7064446806907654)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('jawaharlal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>Extract word ector using pre trained using W2V and fast trained model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:40:03,529 :INFO :loading projection weights from C:\\Users\\Niraj Kumar\\gensim-data\\fasttext-wiki-news-subwords-300\\fasttext-wiki-news-subwords-300.gz\n",
      "2020-08-02 12:46:25,536 :INFO :loaded (999999, 300) matrix from C:\\Users\\Niraj Kumar\\gensim-data\\fasttext-wiki-news-subwords-300\\fasttext-wiki-news-subwords-300.gz\n"
     ]
    }
   ],
   "source": [
    "fasttext_model1300 =api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 12:46:33,952 :INFO :loading projection weights from C:\\Users\\Niraj Kumar\\gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-08-02 12:52:09,096 :INFO :loaded (3000000, 300) matrix from C:\\Users\\Niraj Kumar\\gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "word2vec_model1300=api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 13:43:33,493 :INFO :precomputing L2-norms of word weight vectors\n",
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dogs', 0.8680490255355835),\n",
       " ('puppy', 0.8106428384780884),\n",
       " ('pit_bull', 0.780396044254303),\n",
       " ('pooch', 0.7627376317977905),\n",
       " ('cat', 0.7609457969665527),\n",
       " ('golden_retriever', 0.7500901818275452),\n",
       " ('German_shepherd', 0.7465174198150635),\n",
       " ('Rottweiler', 0.7437615394592285),\n",
       " ('beagle', 0.7418621778488159),\n",
       " ('pup', 0.7406911253929138)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model1300.most_similar('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style='color:red'>Document to Vector: Doc2Vec</span>\n",
    "- Two popular method : a. PV-DM (paragraph vector distributed memory) b. DBOW: Distributed bag of word\n",
    "- PV-DM : Pragraph id + context words ==> Summarzier/average/Concatenate ==> Classifier ==> Target Word .This way we learn weights for Paragraph id and words\n",
    "- DBOW : Paragraph id ==> context words, Learn document vecor by sampling words from the document  \n",
    "- __Document Vector is not simply the average of words vectors in the document__\n",
    "\n",
    "\n",
    "__Workflow:-__\n",
    "- Create training data set by calling function that takes corpus and number to tag the document\n",
    "- This tagged dataset is the tarining set\n",
    "- Call doc2vec and build the model by running through this training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exercise\n",
    "# Read from Text8 and train Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [d for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tagged dataset for training the model\n",
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i , list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words,[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:30:37,299 :WARNING :consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "#Now train the doc2vec model\n",
    "# 1. Intialize Doc2Vec mode;\n",
    "# 2. Build vocabulary\n",
    "# 3. Train the model\n",
    "\n",
    "model=gensim.models.doc2vec.Doc2Vec(vector_size=50,min_count=2,epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:31:32,127 :INFO :collecting all words and their counts\n",
      "2020-08-02 16:31:32,130 :INFO :PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-08-02 16:31:35,917 :INFO :collected 253854 word types and 1701 unique tags from a corpus of 1701 examples and 17005207 words\n",
      "2020-08-02 16:31:35,984 :INFO :Loading a fresh vocabulary\n",
      "2020-08-02 16:37:42,393 :INFO :effective_min_count=2 retains 135335 unique words (53% of original 253854, drops 118519)\n",
      "2020-08-02 16:37:42,395 :INFO :effective_min_count=2 leaves 16886688 word corpus (99% of original 17005207, drops 118519)\n",
      "2020-08-02 16:37:43,323 :INFO :deleting the raw counts dictionary of 253854 items\n",
      "2020-08-02 16:37:43,330 :INFO :sample=0.001 downsamples 37 most-common words\n",
      "2020-08-02 16:37:43,332 :INFO :downsampling leaves estimated 12689806 word corpus (75.1% of prior 16886688)\n",
      "2020-08-02 16:37:44,282 :INFO :estimated required memory for 135335 words and 50 dimensions: 122141700 bytes\n",
      "2020-08-02 16:37:44,316 :INFO :resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:39:31,856 :INFO :training model with 3 workers on 135335 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-08-02 16:39:35,390 :INFO :EPOCH 1 - PROGRESS: at 4.82% examples, 607826 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:36,396 :INFO :EPOCH 1 - PROGRESS: at 10.46% examples, 655463 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:39:37,400 :INFO :EPOCH 1 - PROGRESS: at 16.75% examples, 701219 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:38,417 :INFO :EPOCH 1 - PROGRESS: at 22.69% examples, 711368 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:39,418 :INFO :EPOCH 1 - PROGRESS: at 28.98% examples, 730092 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:40,423 :INFO :EPOCH 1 - PROGRESS: at 34.86% examples, 733877 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:41,436 :INFO :EPOCH 1 - PROGRESS: at 41.21% examples, 742839 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:42,439 :INFO :EPOCH 1 - PROGRESS: at 46.97% examples, 741729 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:43,440 :INFO :EPOCH 1 - PROGRESS: at 53.32% examples, 749215 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:44,441 :INFO :EPOCH 1 - PROGRESS: at 59.67% examples, 755148 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:39:45,443 :INFO :EPOCH 1 - PROGRESS: at 65.96% examples, 758960 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:46,455 :INFO :EPOCH 1 - PROGRESS: at 72.43% examples, 763553 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:39:47,460 :INFO :EPOCH 1 - PROGRESS: at 78.37% examples, 761378 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:39:48,465 :INFO :EPOCH 1 - PROGRESS: at 84.71% examples, 764108 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:49,467 :INFO :EPOCH 1 - PROGRESS: at 91.06% examples, 766881 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:50,482 :INFO :EPOCH 1 - PROGRESS: at 97.35% examples, 767677 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:50,883 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:39:50,894 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:39:50,895 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:39:50,897 :INFO :EPOCH - 1 : training on 17005207 raw words (12691506 effective words) took 16.5s, 768569 effective words/s\n",
      "2020-08-02 16:39:52,385 :INFO :EPOCH 2 - PROGRESS: at 6.23% examples, 785945 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:53,390 :INFO :EPOCH 2 - PROGRESS: at 12.64% examples, 795537 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:54,404 :INFO :EPOCH 2 - PROGRESS: at 19.05% examples, 797060 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:55,405 :INFO :EPOCH 2 - PROGRESS: at 25.34% examples, 797908 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:56,407 :INFO :EPOCH 2 - PROGRESS: at 31.57% examples, 798391 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:57,411 :INFO :EPOCH 2 - PROGRESS: at 37.92% examples, 800131 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:58,413 :INFO :EPOCH 2 - PROGRESS: at 43.80% examples, 792346 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:39:59,414 :INFO :EPOCH 2 - PROGRESS: at 50.09% examples, 793733 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:00,431 :INFO :EPOCH 2 - PROGRESS: at 56.26% examples, 791734 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:01,434 :INFO :EPOCH 2 - PROGRESS: at 62.55% examples, 792119 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:02,444 :INFO :EPOCH 2 - PROGRESS: at 68.72% examples, 790733 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:03,449 :INFO :EPOCH 2 - PROGRESS: at 75.01% examples, 791028 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:04,452 :INFO :EPOCH 2 - PROGRESS: at 81.48% examples, 791912 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:05,455 :INFO :EPOCH 2 - PROGRESS: at 87.89% examples, 793199 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:06,456 :INFO :EPOCH 2 - PROGRESS: at 94.24% examples, 793921 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:40:07,364 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:40:07,365 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:40:07,376 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:40:07,378 :INFO :EPOCH - 2 : training on 17005207 raw words (12691754 effective words) took 16.0s, 793598 effective words/s\n",
      "2020-08-02 16:40:08,396 :INFO :EPOCH 3 - PROGRESS: at 6.29% examples, 783357 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:09,397 :INFO :EPOCH 3 - PROGRESS: at 12.40% examples, 776735 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:10,400 :INFO :EPOCH 3 - PROGRESS: at 18.93% examples, 792269 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:11,404 :INFO :EPOCH 3 - PROGRESS: at 25.40% examples, 799386 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:12,405 :INFO :EPOCH 3 - PROGRESS: at 31.75% examples, 802557 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:13,406 :INFO :EPOCH 3 - PROGRESS: at 38.10% examples, 804277 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:14,413 :INFO :EPOCH 3 - PROGRESS: at 44.39% examples, 802722 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:15,431 :INFO :EPOCH 3 - PROGRESS: at 50.85% examples, 803875 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:16,433 :INFO :EPOCH 3 - PROGRESS: at 57.14% examples, 803362 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:17,440 :INFO :EPOCH 3 - PROGRESS: at 63.43% examples, 802653 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:40:18,448 :INFO :EPOCH 3 - PROGRESS: at 68.78% examples, 790957 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:40:19,460 :INFO :EPOCH 3 - PROGRESS: at 74.96% examples, 789553 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:20,468 :INFO :EPOCH 3 - PROGRESS: at 81.31% examples, 789091 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:21,484 :INFO :EPOCH 3 - PROGRESS: at 87.60% examples, 788869 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:22,500 :INFO :EPOCH 3 - PROGRESS: at 94.00% examples, 789470 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:23,470 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:40:23,472 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:40:23,482 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:40:23,483 :INFO :EPOCH - 3 : training on 17005207 raw words (12691209 effective words) took 16.1s, 788203 effective words/s\n",
      "2020-08-02 16:40:24,503 :INFO :EPOCH 4 - PROGRESS: at 6.17% examples, 768540 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:40:25,506 :INFO :EPOCH 4 - PROGRESS: at 12.23% examples, 765249 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:26,510 :INFO :EPOCH 4 - PROGRESS: at 18.22% examples, 762438 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:27,516 :INFO :EPOCH 4 - PROGRESS: at 24.22% examples, 761748 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:28,528 :INFO :EPOCH 4 - PROGRESS: at 30.04% examples, 756878 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:40:29,528 :INFO :EPOCH 4 - PROGRESS: at 36.04% examples, 758886 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:30,546 :INFO :EPOCH 4 - PROGRESS: at 42.15% examples, 759589 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:31,550 :INFO :EPOCH 4 - PROGRESS: at 48.15% examples, 759935 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:32,554 :INFO :EPOCH 4 - PROGRESS: at 54.20% examples, 760999 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:33,560 :INFO :EPOCH 4 - PROGRESS: at 60.14% examples, 760003 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:34,563 :INFO :EPOCH 4 - PROGRESS: at 66.08% examples, 759299 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:35,576 :INFO :EPOCH 4 - PROGRESS: at 72.02% examples, 758165 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:40:36,578 :INFO :EPOCH 4 - PROGRESS: at 78.01% examples, 757192 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:37,579 :INFO :EPOCH 4 - PROGRESS: at 83.95% examples, 756928 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:38,584 :INFO :EPOCH 4 - PROGRESS: at 89.77% examples, 755515 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:39,591 :INFO :EPOCH 4 - PROGRESS: at 95.94% examples, 756618 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:40:40,275 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:40:40,276 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:40:40,290 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:40:40,291 :INFO :EPOCH - 4 : training on 17005207 raw words (12692621 effective words) took 16.8s, 755425 effective words/s\n",
      "2020-08-02 16:40:41,309 :INFO :EPOCH 5 - PROGRESS: at 6.00% examples, 748610 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:40:42,320 :INFO :EPOCH 5 - PROGRESS: at 12.11% examples, 755259 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:43,328 :INFO :EPOCH 5 - PROGRESS: at 18.28% examples, 762098 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:44,335 :INFO :EPOCH 5 - PROGRESS: at 24.34% examples, 763090 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:45,344 :INFO :EPOCH 5 - PROGRESS: at 30.45% examples, 766094 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:46,345 :INFO :EPOCH 5 - PROGRESS: at 36.39% examples, 765324 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:47,356 :INFO :EPOCH 5 - PROGRESS: at 42.50% examples, 765753 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:40:48,356 :INFO :EPOCH 5 - PROGRESS: at 48.50% examples, 765601 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:49,362 :INFO :EPOCH 5 - PROGRESS: at 54.56% examples, 766115 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:50,367 :INFO :EPOCH 5 - PROGRESS: at 60.49% examples, 764636 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:51,367 :INFO :EPOCH 5 - PROGRESS: at 66.55% examples, 764870 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:52,383 :INFO :EPOCH 5 - PROGRESS: at 72.60% examples, 764473 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:53,384 :INFO :EPOCH 5 - PROGRESS: at 78.72% examples, 764106 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:40:54,394 :INFO :EPOCH 5 - PROGRESS: at 84.36% examples, 760093 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:55,410 :INFO :EPOCH 5 - PROGRESS: at 90.48% examples, 760520 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:56,416 :INFO :EPOCH 5 - PROGRESS: at 96.41% examples, 759450 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:56,994 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:40:56,995 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:40:57,006 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:40:57,007 :INFO :EPOCH - 5 : training on 17005207 raw words (12692976 effective words) took 16.7s, 759522 effective words/s\n",
      "2020-08-02 16:40:58,013 :INFO :EPOCH 6 - PROGRESS: at 5.82% examples, 735810 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:40:59,014 :INFO :EPOCH 6 - PROGRESS: at 11.93% examples, 752067 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:00,015 :INFO :EPOCH 6 - PROGRESS: at 17.64% examples, 742208 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:01,020 :INFO :EPOCH 6 - PROGRESS: at 23.34% examples, 737106 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:02,037 :INFO :EPOCH 6 - PROGRESS: at 28.87% examples, 728756 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:41:03,041 :INFO :EPOCH 6 - PROGRESS: at 34.51% examples, 727780 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:04,052 :INFO :EPOCH 6 - PROGRESS: at 39.39% examples, 711561 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:05,060 :INFO :EPOCH 6 - PROGRESS: at 45.15% examples, 713578 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:06,060 :INFO :EPOCH 6 - PROGRESS: at 50.73% examples, 713625 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:07,080 :INFO :EPOCH 6 - PROGRESS: at 56.20% examples, 710734 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:08,081 :INFO :EPOCH 6 - PROGRESS: at 61.14% examples, 703198 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:09,083 :INFO :EPOCH 6 - PROGRESS: at 66.84% examples, 704644 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:10,093 :INFO :EPOCH 6 - PROGRESS: at 72.49% examples, 705163 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:11,109 :INFO :EPOCH 6 - PROGRESS: at 78.66% examples, 708878 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:12,111 :INFO :EPOCH 6 - PROGRESS: at 84.66% examples, 712087 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:13,122 :INFO :EPOCH 6 - PROGRESS: at 90.71% examples, 715226 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:14,141 :INFO :EPOCH 6 - PROGRESS: at 96.71% examples, 716843 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:41:14,820 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:41:14,834 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:41:14,847 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:41:14,849 :INFO :EPOCH - 6 : training on 17005207 raw words (12691476 effective words) took 17.8s, 711557 effective words/s\n",
      "2020-08-02 16:41:15,859 :INFO :EPOCH 7 - PROGRESS: at 5.64% examples, 712465 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:41:16,866 :INFO :EPOCH 7 - PROGRESS: at 11.58% examples, 726167 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:17,868 :INFO :EPOCH 7 - PROGRESS: at 17.52% examples, 734850 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:18,869 :INFO :EPOCH 7 - PROGRESS: at 23.28% examples, 734189 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:41:19,869 :INFO :EPOCH 7 - PROGRESS: at 28.92% examples, 731804 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:20,879 :INFO :EPOCH 7 - PROGRESS: at 34.74% examples, 733560 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:21,883 :INFO :EPOCH 7 - PROGRESS: at 40.45% examples, 732029 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:22,888 :INFO :EPOCH 7 - PROGRESS: at 46.27% examples, 732868 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:23,909 :INFO :EPOCH 7 - PROGRESS: at 52.03% examples, 731197 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:24,927 :INFO :EPOCH 7 - PROGRESS: at 57.85% examples, 731167 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:25,932 :INFO :EPOCH 7 - PROGRESS: at 63.61% examples, 731058 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:26,937 :INFO :EPOCH 7 - PROGRESS: at 68.78% examples, 724624 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:27,941 :INFO :EPOCH 7 - PROGRESS: at 74.49% examples, 724542 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:28,959 :INFO :EPOCH 7 - PROGRESS: at 80.01% examples, 720669 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:29,969 :INFO :EPOCH 7 - PROGRESS: at 85.48% examples, 718375 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:30,986 :INFO :EPOCH 7 - PROGRESS: at 91.48% examples, 720265 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:31,993 :INFO :EPOCH 7 - PROGRESS: at 97.30% examples, 720783 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:32,458 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:41:32,460 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:41:32,472 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:41:32,475 :INFO :EPOCH - 7 : training on 17005207 raw words (12692312 effective words) took 17.6s, 720394 effective words/s\n",
      "2020-08-02 16:41:33,510 :INFO :EPOCH 8 - PROGRESS: at 5.94% examples, 744827 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:34,512 :INFO :EPOCH 8 - PROGRESS: at 11.58% examples, 726276 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:41:35,524 :INFO :EPOCH 8 - PROGRESS: at 17.11% examples, 715014 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:36,537 :INFO :EPOCH 8 - PROGRESS: at 22.34% examples, 699823 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:37,542 :INFO :EPOCH 8 - PROGRESS: at 26.98% examples, 678474 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:41:38,554 :INFO :EPOCH 8 - PROGRESS: at 30.81% examples, 646222 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:39,566 :INFO :EPOCH 8 - PROGRESS: at 36.04% examples, 648773 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:40,570 :INFO :EPOCH 8 - PROGRESS: at 41.80% examples, 658714 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:41,571 :INFO :EPOCH 8 - PROGRESS: at 47.44% examples, 665337 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:42,574 :INFO :EPOCH 8 - PROGRESS: at 53.38% examples, 674501 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:41:43,575 :INFO :EPOCH 8 - PROGRESS: at 59.20% examples, 680413 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:44,575 :INFO :EPOCH 8 - PROGRESS: at 65.20% examples, 687137 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:45,577 :INFO :EPOCH 8 - PROGRESS: at 70.96% examples, 690682 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:46,582 :INFO :EPOCH 8 - PROGRESS: at 77.01% examples, 695041 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:41:47,590 :INFO :EPOCH 8 - PROGRESS: at 83.01% examples, 698673 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:48,591 :INFO :EPOCH 8 - PROGRESS: at 89.01% examples, 702603 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:49,601 :INFO :EPOCH 8 - PROGRESS: at 95.24% examples, 707172 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:50,369 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:41:50,379 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:41:50,389 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:41:50,392 :INFO :EPOCH - 8 : training on 17005207 raw words (12690761 effective words) took 17.9s, 709414 effective words/s\n",
      "2020-08-02 16:41:51,403 :INFO :EPOCH 9 - PROGRESS: at 6.06% examples, 759775 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:52,406 :INFO :EPOCH 9 - PROGRESS: at 12.23% examples, 767804 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:53,418 :INFO :EPOCH 9 - PROGRESS: at 18.34% examples, 767281 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:54,421 :INFO :EPOCH 9 - PROGRESS: at 23.46% examples, 737629 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:55,433 :INFO :EPOCH 9 - PROGRESS: at 29.34% examples, 738743 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:41:56,438 :INFO :EPOCH 9 - PROGRESS: at 35.45% examples, 746035 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:57,442 :INFO :EPOCH 9 - PROGRESS: at 41.50% examples, 748988 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:58,455 :INFO :EPOCH 9 - PROGRESS: at 47.74% examples, 753532 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:41:59,461 :INFO :EPOCH 9 - PROGRESS: at 53.73% examples, 754297 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:00,468 :INFO :EPOCH 9 - PROGRESS: at 59.73% examples, 754767 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:01,474 :INFO :EPOCH 9 - PROGRESS: at 65.84% examples, 756212 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:02,474 :INFO :EPOCH 9 - PROGRESS: at 71.78% examples, 756153 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:03,487 :INFO :EPOCH 9 - PROGRESS: at 77.90% examples, 755892 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:04,491 :INFO :EPOCH 9 - PROGRESS: at 83.95% examples, 756540 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:05,499 :INFO :EPOCH 9 - PROGRESS: at 90.12% examples, 758065 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:06,505 :INFO :EPOCH 9 - PROGRESS: at 96.30% examples, 759047 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:07,104 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:42:07,106 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:42:07,113 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:42:07,116 :INFO :EPOCH - 9 : training on 17005207 raw words (12691726 effective words) took 16.7s, 759116 effective words/s\n",
      "2020-08-02 16:42:08,151 :INFO :EPOCH 10 - PROGRESS: at 6.17% examples, 778329 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:09,167 :INFO :EPOCH 10 - PROGRESS: at 12.29% examples, 768484 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:10,171 :INFO :EPOCH 10 - PROGRESS: at 18.46% examples, 772292 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:11,176 :INFO :EPOCH 10 - PROGRESS: at 24.22% examples, 761731 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:12,180 :INFO :EPOCH 10 - PROGRESS: at 29.86% examples, 753717 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:13,188 :INFO :EPOCH 10 - PROGRESS: at 35.51% examples, 747741 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:14,194 :INFO :EPOCH 10 - PROGRESS: at 41.56% examples, 750130 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:15,211 :INFO :EPOCH 10 - PROGRESS: at 47.68% examples, 752229 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:16,220 :INFO :EPOCH 10 - PROGRESS: at 53.67% examples, 752933 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:17,223 :INFO :EPOCH 10 - PROGRESS: at 59.44% examples, 750874 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:18,227 :INFO :EPOCH 10 - PROGRESS: at 65.55% examples, 752879 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:19,233 :INFO :EPOCH 10 - PROGRESS: at 71.13% examples, 749171 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:20,235 :INFO :EPOCH 10 - PROGRESS: at 77.37% examples, 751166 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:21,235 :INFO :EPOCH 10 - PROGRESS: at 83.42% examples, 752189 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:22,239 :INFO :EPOCH 10 - PROGRESS: at 89.59% examples, 754206 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:23,241 :INFO :EPOCH 10 - PROGRESS: at 95.65% examples, 754727 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:23,940 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:42:23,954 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:42:23,955 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:42:23,957 :INFO :EPOCH - 10 : training on 17005207 raw words (12689674 effective words) took 16.8s, 755069 effective words/s\n",
      "2020-08-02 16:42:24,983 :INFO :EPOCH 11 - PROGRESS: at 6.00% examples, 742464 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:25,986 :INFO :EPOCH 11 - PROGRESS: at 12.11% examples, 755081 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:26,995 :INFO :EPOCH 11 - PROGRESS: at 18.28% examples, 761698 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:28,004 :INFO :EPOCH 11 - PROGRESS: at 24.34% examples, 762399 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:29,013 :INFO :EPOCH 11 - PROGRESS: at 30.39% examples, 763693 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:30,023 :INFO :EPOCH 11 - PROGRESS: at 36.33% examples, 762330 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:31,033 :INFO :EPOCH 11 - PROGRESS: at 41.98% examples, 754803 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:42:32,039 :INFO :EPOCH 11 - PROGRESS: at 47.97% examples, 755272 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:33,041 :INFO :EPOCH 11 - PROGRESS: at 53.97% examples, 756368 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:34,051 :INFO :EPOCH 11 - PROGRESS: at 60.02% examples, 757060 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:35,051 :INFO :EPOCH 11 - PROGRESS: at 66.14% examples, 758760 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:36,052 :INFO :EPOCH 11 - PROGRESS: at 72.13% examples, 759086 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:37,061 :INFO :EPOCH 11 - PROGRESS: at 78.31% examples, 759304 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:38,079 :INFO :EPOCH 11 - PROGRESS: at 84.48% examples, 759894 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:39,083 :INFO :EPOCH 11 - PROGRESS: at 90.53% examples, 760489 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:40,093 :INFO :EPOCH 11 - PROGRESS: at 96.77% examples, 761466 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:40,601 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:42:40,612 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:42:40,619 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:42:40,621 :INFO :EPOCH - 11 : training on 17005207 raw words (12688528 effective words) took 16.7s, 761686 effective words/s\n",
      "2020-08-02 16:42:41,631 :INFO :EPOCH 12 - PROGRESS: at 6.00% examples, 754948 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:42,635 :INFO :EPOCH 12 - PROGRESS: at 12.29% examples, 772052 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:43,654 :INFO :EPOCH 12 - PROGRESS: at 18.58% examples, 775371 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:44,662 :INFO :EPOCH 12 - PROGRESS: at 24.28% examples, 761896 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:42:45,668 :INFO :EPOCH 12 - PROGRESS: at 29.86% examples, 752041 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:42:46,680 :INFO :EPOCH 12 - PROGRESS: at 35.04% examples, 735894 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:47,691 :INFO :EPOCH 12 - PROGRESS: at 40.51% examples, 729176 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:48,693 :INFO :EPOCH 12 - PROGRESS: at 45.91% examples, 723966 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:49,697 :INFO :EPOCH 12 - PROGRESS: at 51.50% examples, 722375 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:50,702 :INFO :EPOCH 12 - PROGRESS: at 57.44% examples, 725466 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:51,714 :INFO :EPOCH 12 - PROGRESS: at 63.20% examples, 725495 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:52,721 :INFO :EPOCH 12 - PROGRESS: at 69.37% examples, 729934 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:53,735 :INFO :EPOCH 12 - PROGRESS: at 75.66% examples, 733830 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:54,744 :INFO :EPOCH 12 - PROGRESS: at 81.78% examples, 735744 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:55,747 :INFO :EPOCH 12 - PROGRESS: at 87.89% examples, 738166 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:56,755 :INFO :EPOCH 12 - PROGRESS: at 94.00% examples, 740011 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:57,717 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:42:57,727 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:42:57,734 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:42:57,736 :INFO :EPOCH - 12 : training on 17005207 raw words (12692486 effective words) took 17.1s, 741873 effective words/s\n",
      "2020-08-02 16:42:58,754 :INFO :EPOCH 13 - PROGRESS: at 6.11% examples, 763219 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:42:59,769 :INFO :EPOCH 13 - PROGRESS: at 12.29% examples, 764812 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:43:00,770 :INFO :EPOCH 13 - PROGRESS: at 17.99% examples, 750913 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:01,790 :INFO :EPOCH 13 - PROGRESS: at 23.75% examples, 743094 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:43:02,927 :INFO :EPOCH 13 - PROGRESS: at 29.28% examples, 716402 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:04,004 :INFO :EPOCH 13 - PROGRESS: at 29.81% examples, 604361 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:05,010 :INFO :EPOCH 13 - PROGRESS: at 35.10% examples, 613926 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:06,018 :INFO :EPOCH 13 - PROGRESS: at 41.33% examples, 634937 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:07,018 :INFO :EPOCH 13 - PROGRESS: at 47.50% examples, 651321 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:08,033 :INFO :EPOCH 13 - PROGRESS: at 53.73% examples, 664399 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:09,039 :INFO :EPOCH 13 - PROGRESS: at 59.79% examples, 673531 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:10,045 :INFO :EPOCH 13 - PROGRESS: at 65.96% examples, 682178 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:11,056 :INFO :EPOCH 13 - PROGRESS: at 72.08% examples, 688832 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:12,060 :INFO :EPOCH 13 - PROGRESS: at 78.19% examples, 693705 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:13,062 :INFO :EPOCH 13 - PROGRESS: at 84.30% examples, 698921 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:14,063 :INFO :EPOCH 13 - PROGRESS: at 90.36% examples, 703250 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:15,068 :INFO :EPOCH 13 - PROGRESS: at 96.53% examples, 707295 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:15,617 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:43:15,626 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:43:15,671 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:43:15,673 :INFO :EPOCH - 13 : training on 17005207 raw words (12690342 effective words) took 17.9s, 707770 effective words/s\n",
      "2020-08-02 16:43:16,680 :INFO :EPOCH 14 - PROGRESS: at 6.17% examples, 777515 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:17,685 :INFO :EPOCH 14 - PROGRESS: at 12.46% examples, 783403 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:18,688 :INFO :EPOCH 14 - PROGRESS: at 18.52% examples, 777292 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:19,698 :INFO :EPOCH 14 - PROGRESS: at 24.75% examples, 779119 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:20,704 :INFO :EPOCH 14 - PROGRESS: at 30.81% examples, 778284 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:21,707 :INFO :EPOCH 14 - PROGRESS: at 36.86% examples, 777561 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:22,713 :INFO :EPOCH 14 - PROGRESS: at 42.92% examples, 775865 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:23,713 :INFO :EPOCH 14 - PROGRESS: at 48.97% examples, 775370 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:24,715 :INFO :EPOCH 14 - PROGRESS: at 54.67% examples, 770149 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:25,725 :INFO :EPOCH 14 - PROGRESS: at 60.91% examples, 771633 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:26,737 :INFO :EPOCH 14 - PROGRESS: at 66.73% examples, 767756 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:27,743 :INFO :EPOCH 14 - PROGRESS: at 72.84% examples, 768389 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:28,749 :INFO :EPOCH 14 - PROGRESS: at 79.01% examples, 767926 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:29,757 :INFO :EPOCH 14 - PROGRESS: at 85.19% examples, 768399 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:30,766 :INFO :EPOCH 14 - PROGRESS: at 91.18% examples, 767648 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:31,769 :INFO :EPOCH 14 - PROGRESS: at 97.35% examples, 768102 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:32,210 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:43:32,213 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:43:32,218 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:43:32,219 :INFO :EPOCH - 14 : training on 17005207 raw words (12692565 effective words) took 16.5s, 767328 effective words/s\n",
      "2020-08-02 16:43:33,228 :INFO :EPOCH 15 - PROGRESS: at 6.06% examples, 761838 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:34,247 :INFO :EPOCH 15 - PROGRESS: at 12.29% examples, 766463 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:35,254 :INFO :EPOCH 15 - PROGRESS: at 18.52% examples, 772642 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:36,254 :INFO :EPOCH 15 - PROGRESS: at 24.57% examples, 772071 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:37,264 :INFO :EPOCH 15 - PROGRESS: at 30.57% examples, 770072 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:38,271 :INFO :EPOCH 15 - PROGRESS: at 36.68% examples, 771569 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:39,279 :INFO :EPOCH 15 - PROGRESS: at 42.74% examples, 770492 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:40,281 :INFO :EPOCH 15 - PROGRESS: at 48.85% examples, 771360 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:41,293 :INFO :EPOCH 15 - PROGRESS: at 55.03% examples, 772305 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:42,296 :INFO :EPOCH 15 - PROGRESS: at 61.14% examples, 772816 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:43,297 :INFO :EPOCH 15 - PROGRESS: at 67.31% examples, 773574 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:44,298 :INFO :EPOCH 15 - PROGRESS: at 73.37% examples, 773445 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:45,305 :INFO :EPOCH 15 - PROGRESS: at 79.01% examples, 767355 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:46,311 :INFO :EPOCH 15 - PROGRESS: at 85.01% examples, 766415 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:47,330 :INFO :EPOCH 15 - PROGRESS: at 91.24% examples, 767157 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:48,336 :INFO :EPOCH 15 - PROGRESS: at 97.30% examples, 766643 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:48,759 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:43:48,768 :INFO :worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:43:48,773 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:43:48,774 :INFO :EPOCH - 15 : training on 17005207 raw words (12691700 effective words) took 16.5s, 766915 effective words/s\n",
      "2020-08-02 16:43:49,786 :INFO :EPOCH 16 - PROGRESS: at 6.17% examples, 773311 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:50,792 :INFO :EPOCH 16 - PROGRESS: at 12.23% examples, 765794 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:51,798 :INFO :EPOCH 16 - PROGRESS: at 18.40% examples, 769628 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:52,803 :INFO :EPOCH 16 - PROGRESS: at 24.10% examples, 758462 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:53,809 :INFO :EPOCH 16 - PROGRESS: at 29.57% examples, 746173 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:54,812 :INFO :EPOCH 16 - PROGRESS: at 34.27% examples, 722095 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:55,815 :INFO :EPOCH 16 - PROGRESS: at 39.09% examples, 706511 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:56,817 :INFO :EPOCH 16 - PROGRESS: at 43.62% examples, 690106 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:43:57,837 :INFO :EPOCH 16 - PROGRESS: at 48.50% examples, 681127 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:58,850 :INFO :EPOCH 16 - PROGRESS: at 54.61% examples, 690304 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:43:59,855 :INFO :EPOCH 16 - PROGRESS: at 60.73% examples, 697934 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:00,871 :INFO :EPOCH 16 - PROGRESS: at 66.78% examples, 702844 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:01,874 :INFO :EPOCH 16 - PROGRESS: at 72.49% examples, 704428 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:44:02,889 :INFO :EPOCH 16 - PROGRESS: at 78.66% examples, 708302 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:03,892 :INFO :EPOCH 16 - PROGRESS: at 84.60% examples, 711017 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:04,895 :INFO :EPOCH 16 - PROGRESS: at 90.59% examples, 714118 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:05,897 :INFO :EPOCH 16 - PROGRESS: at 96.71% examples, 717357 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:06,452 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:44:06,455 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:44:06,461 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:44:06,462 :INFO :EPOCH - 16 : training on 17005207 raw words (12693109 effective words) took 17.7s, 717790 effective words/s\n",
      "2020-08-02 16:44:07,473 :INFO :EPOCH 17 - PROGRESS: at 4.88% examples, 615330 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:08,474 :INFO :EPOCH 17 - PROGRESS: at 11.11% examples, 697195 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:09,493 :INFO :EPOCH 17 - PROGRESS: at 17.28% examples, 721355 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:10,495 :INFO :EPOCH 17 - PROGRESS: at 23.52% examples, 738833 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:11,506 :INFO :EPOCH 17 - PROGRESS: at 29.45% examples, 741454 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:12,512 :INFO :EPOCH 17 - PROGRESS: at 35.63% examples, 749243 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:13,533 :INFO :EPOCH 17 - PROGRESS: at 41.74% examples, 751044 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:14,539 :INFO :EPOCH 17 - PROGRESS: at 47.91% examples, 755069 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:15,549 :INFO :EPOCH 17 - PROGRESS: at 54.09% examples, 757889 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:16,550 :INFO :EPOCH 17 - PROGRESS: at 60.14% examples, 759105 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:17,565 :INFO :EPOCH 17 - PROGRESS: at 66.31% examples, 760321 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:18,575 :INFO :EPOCH 17 - PROGRESS: at 72.25% examples, 759302 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:19,577 :INFO :EPOCH 17 - PROGRESS: at 78.42% examples, 759859 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:20,578 :INFO :EPOCH 17 - PROGRESS: at 84.48% examples, 760284 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:21,588 :INFO :EPOCH 17 - PROGRESS: at 90.12% examples, 757087 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:22,665 :INFO :EPOCH 17 - PROGRESS: at 95.83% examples, 751153 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:23,685 :INFO :EPOCH 17 - PROGRESS: at 96.71% examples, 713049 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:24,693 :INFO :EPOCH 17 - PROGRESS: at 97.94% examples, 682034 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:25,700 :INFO :EPOCH 17 - PROGRESS: at 98.88% examples, 652527 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:26,419 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:44:26,422 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:44:26,452 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:44:26,458 :INFO :EPOCH - 17 : training on 17005207 raw words (12690506 effective words) took 20.0s, 634804 effective words/s\n",
      "2020-08-02 16:44:27,475 :INFO :EPOCH 18 - PROGRESS: at 1.35% examples, 172654 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:28,504 :INFO :EPOCH 18 - PROGRESS: at 2.94% examples, 185033 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:29,540 :INFO :EPOCH 18 - PROGRESS: at 4.70% examples, 194734 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:30,564 :INFO :EPOCH 18 - PROGRESS: at 6.53% examples, 201012 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:31,587 :INFO :EPOCH 18 - PROGRESS: at 8.23% examples, 202878 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:32,591 :INFO :EPOCH 18 - PROGRESS: at 9.82% examples, 202186 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:33,619 :INFO :EPOCH 18 - PROGRESS: at 11.58% examples, 204231 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:34,624 :INFO :EPOCH 18 - PROGRESS: at 13.29% examples, 205634 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:35,640 :INFO :EPOCH 18 - PROGRESS: at 15.23% examples, 209570 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:36,650 :INFO :EPOCH 18 - PROGRESS: at 17.17% examples, 212986 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:37,669 :INFO :EPOCH 18 - PROGRESS: at 19.22% examples, 216858 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:38,673 :INFO :EPOCH 18 - PROGRESS: at 21.28% examples, 220423 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:39,677 :INFO :EPOCH 18 - PROGRESS: at 23.63% examples, 226518 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:40,691 :INFO :EPOCH 18 - PROGRESS: at 26.04% examples, 231975 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:41,737 :INFO :EPOCH 18 - PROGRESS: at 28.57% examples, 237305 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:42,756 :INFO :EPOCH 18 - PROGRESS: at 31.22% examples, 243410 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:43,788 :INFO :EPOCH 18 - PROGRESS: at 34.04% examples, 249805 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:44,799 :INFO :EPOCH 18 - PROGRESS: at 36.98% examples, 256540 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:45,805 :INFO :EPOCH 18 - PROGRESS: at 39.86% examples, 262127 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:46,817 :INFO :EPOCH 18 - PROGRESS: at 43.03% examples, 269023 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:47,836 :INFO :EPOCH 18 - PROGRESS: at 46.21% examples, 275083 words/s, in_qsize 5, out_qsize 1\n",
      "2020-08-02 16:44:48,851 :INFO :EPOCH 18 - PROGRESS: at 49.38% examples, 280734 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:49,858 :INFO :EPOCH 18 - PROGRESS: at 52.62% examples, 286238 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:50,880 :INFO :EPOCH 18 - PROGRESS: at 55.38% examples, 288796 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:51,881 :INFO :EPOCH 18 - PROGRESS: at 58.55% examples, 293267 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:52,884 :INFO :EPOCH 18 - PROGRESS: at 64.49% examples, 310779 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:53,891 :INFO :EPOCH 18 - PROGRESS: at 70.78% examples, 328540 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:54,906 :INFO :EPOCH 18 - PROGRESS: at 76.07% examples, 340155 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:44:55,907 :INFO :EPOCH 18 - PROGRESS: at 81.07% examples, 349856 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:56,913 :INFO :EPOCH 18 - PROGRESS: at 86.42% examples, 360607 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:44:57,914 :INFO :EPOCH 18 - PROGRESS: at 91.42% examples, 369282 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:44:58,916 :INFO :EPOCH 18 - PROGRESS: at 97.06% examples, 379829 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:44:59,391 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:44:59,396 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:44:59,406 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:44:59,407 :INFO :EPOCH - 18 : training on 17005207 raw words (12692486 effective words) took 32.9s, 385379 effective words/s\n",
      "2020-08-02 16:45:00,415 :INFO :EPOCH 19 - PROGRESS: at 6.11% examples, 772055 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:01,417 :INFO :EPOCH 19 - PROGRESS: at 12.35% examples, 778029 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:02,417 :INFO :EPOCH 19 - PROGRESS: at 18.46% examples, 776538 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:03,431 :INFO :EPOCH 19 - PROGRESS: at 24.51% examples, 772636 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:04,433 :INFO :EPOCH 19 - PROGRESS: at 30.51% examples, 771451 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:05,449 :INFO :EPOCH 19 - PROGRESS: at 36.80% examples, 775282 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:06,455 :INFO :EPOCH 19 - PROGRESS: at 43.21% examples, 780385 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:07,460 :INFO :EPOCH 19 - PROGRESS: at 49.27% examples, 778999 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:08,475 :INFO :EPOCH 19 - PROGRESS: at 55.50% examples, 779575 words/s, in_qsize 5, out_qsize 1\n",
      "2020-08-02 16:45:09,476 :INFO :EPOCH 19 - PROGRESS: at 61.79% examples, 781498 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:10,484 :INFO :EPOCH 19 - PROGRESS: at 68.08% examples, 782508 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:11,485 :INFO :EPOCH 19 - PROGRESS: at 74.54% examples, 785862 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:12,493 :INFO :EPOCH 19 - PROGRESS: at 81.07% examples, 787193 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:13,497 :INFO :EPOCH 19 - PROGRESS: at 87.48% examples, 788826 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:14,511 :INFO :EPOCH 19 - PROGRESS: at 93.89% examples, 789485 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:15,439 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:45:15,450 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:45:15,459 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:45:15,460 :INFO :EPOCH - 19 : training on 17005207 raw words (12690884 effective words) took 16.0s, 790907 effective words/s\n",
      "2020-08-02 16:45:16,479 :INFO :EPOCH 20 - PROGRESS: at 6.23% examples, 778622 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:17,486 :INFO :EPOCH 20 - PROGRESS: at 11.46% examples, 715968 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:18,506 :INFO :EPOCH 20 - PROGRESS: at 17.46% examples, 726046 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:19,510 :INFO :EPOCH 20 - PROGRESS: at 22.81% examples, 713850 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:20,525 :INFO :EPOCH 20 - PROGRESS: at 28.92% examples, 725347 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:21,546 :INFO :EPOCH 20 - PROGRESS: at 35.04% examples, 732742 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:22,548 :INFO :EPOCH 20 - PROGRESS: at 40.80% examples, 732557 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:23,559 :INFO :EPOCH 20 - PROGRESS: at 46.91% examples, 737475 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:24,560 :INFO :EPOCH 20 - PROGRESS: at 53.15% examples, 743763 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:25,563 :INFO :EPOCH 20 - PROGRESS: at 58.91% examples, 742595 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:26,566 :INFO :EPOCH 20 - PROGRESS: at 65.31% examples, 748819 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:27,572 :INFO :EPOCH 20 - PROGRESS: at 71.37% examples, 750354 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:28,576 :INFO :EPOCH 20 - PROGRESS: at 78.01% examples, 755950 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:29,580 :INFO :EPOCH 20 - PROGRESS: at 84.07% examples, 756569 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:30,587 :INFO :EPOCH 20 - PROGRESS: at 90.30% examples, 758692 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:31,602 :INFO :EPOCH 20 - PROGRESS: at 96.00% examples, 755473 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:32,242 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:45:32,244 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:45:32,253 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:45:32,254 :INFO :EPOCH - 20 : training on 17005207 raw words (12690554 effective words) took 16.8s, 755997 effective words/s\n",
      "2020-08-02 16:45:33,274 :INFO :EPOCH 21 - PROGRESS: at 5.76% examples, 719078 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:34,282 :INFO :EPOCH 21 - PROGRESS: at 11.82% examples, 737179 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:35,283 :INFO :EPOCH 21 - PROGRESS: at 17.64% examples, 737239 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:45:36,284 :INFO :EPOCH 21 - PROGRESS: at 22.99% examples, 722617 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:37,287 :INFO :EPOCH 21 - PROGRESS: at 28.45% examples, 717992 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:38,290 :INFO :EPOCH 21 - PROGRESS: at 34.39% examples, 725122 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:45:39,300 :INFO :EPOCH 21 - PROGRESS: at 40.39% examples, 729535 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:40,307 :INFO :EPOCH 21 - PROGRESS: at 45.91% examples, 725944 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:41,318 :INFO :EPOCH 21 - PROGRESS: at 51.62% examples, 725161 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:42,329 :INFO :EPOCH 21 - PROGRESS: at 57.97% examples, 732736 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:43,337 :INFO :EPOCH 21 - PROGRESS: at 64.20% examples, 737712 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:44,345 :INFO :EPOCH 21 - PROGRESS: at 70.19% examples, 739229 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:45,355 :INFO :EPOCH 21 - PROGRESS: at 76.43% examples, 741944 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:46,357 :INFO :EPOCH 21 - PROGRESS: at 82.77% examples, 745795 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:47,368 :INFO :EPOCH 21 - PROGRESS: at 89.12% examples, 749433 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:48,367 :INFO :EPOCH 21 - PROGRESS: at 95.41% examples, 752189 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:49,119 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:45:49,132 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:45:49,144 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:45:49,145 :INFO :EPOCH - 21 : training on 17005207 raw words (12692616 effective words) took 16.9s, 751724 effective words/s\n",
      "2020-08-02 16:45:50,154 :INFO :EPOCH 22 - PROGRESS: at 6.06% examples, 764574 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:51,158 :INFO :EPOCH 22 - PROGRESS: at 12.35% examples, 777420 words/s, in_qsize 5, out_qsize 1\n",
      "2020-08-02 16:45:52,158 :INFO :EPOCH 22 - PROGRESS: at 18.58% examples, 781279 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:53,168 :INFO :EPOCH 22 - PROGRESS: at 24.99% examples, 787717 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:45:54,173 :INFO :EPOCH 22 - PROGRESS: at 31.33% examples, 792845 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:55,174 :INFO :EPOCH 22 - PROGRESS: at 37.45% examples, 790889 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:56,186 :INFO :EPOCH 22 - PROGRESS: at 43.80% examples, 791905 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:45:57,188 :INFO :EPOCH 22 - PROGRESS: at 49.68% examples, 786757 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:45:58,198 :INFO :EPOCH 22 - PROGRESS: at 55.97% examples, 787754 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:45:59,205 :INFO :EPOCH 22 - PROGRESS: at 62.26% examples, 788289 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:00,208 :INFO :EPOCH 22 - PROGRESS: at 68.43% examples, 787683 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:01,222 :INFO :EPOCH 22 - PROGRESS: at 74.60% examples, 786725 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:02,228 :INFO :EPOCH 22 - PROGRESS: at 81.13% examples, 788141 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:03,229 :INFO :EPOCH 22 - PROGRESS: at 87.36% examples, 788193 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:04,245 :INFO :EPOCH 22 - PROGRESS: at 93.77% examples, 788859 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:05,253 :INFO :EPOCH 22 - PROGRESS: at 99.88% examples, 787621 words/s, in_qsize 2, out_qsize 1\n",
      "2020-08-02 16:46:05,255 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:46:05,257 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:46:05,271 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:46:05,273 :INFO :EPOCH - 22 : training on 17005207 raw words (12691418 effective words) took 16.1s, 787391 effective words/s\n",
      "2020-08-02 16:46:06,285 :INFO :EPOCH 23 - PROGRESS: at 6.11% examples, 770639 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:46:07,294 :INFO :EPOCH 23 - PROGRESS: at 12.40% examples, 778446 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:08,296 :INFO :EPOCH 23 - PROGRESS: at 18.34% examples, 769308 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:09,309 :INFO :EPOCH 23 - PROGRESS: at 24.69% examples, 776359 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:10,315 :INFO :EPOCH 23 - PROGRESS: at 30.51% examples, 769731 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:11,331 :INFO :EPOCH 23 - PROGRESS: at 36.51% examples, 767869 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:12,335 :INFO :EPOCH 23 - PROGRESS: at 42.45% examples, 765609 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:13,344 :INFO :EPOCH 23 - PROGRESS: at 48.38% examples, 763625 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:14,353 :INFO :EPOCH 23 - PROGRESS: at 54.67% examples, 767435 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:15,353 :INFO :EPOCH 23 - PROGRESS: at 60.91% examples, 769924 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:16,355 :INFO :EPOCH 23 - PROGRESS: at 67.14% examples, 771537 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:17,361 :INFO :EPOCH 23 - PROGRESS: at 73.25% examples, 771939 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:18,375 :INFO :EPOCH 23 - PROGRESS: at 79.42% examples, 770590 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:19,380 :INFO :EPOCH 23 - PROGRESS: at 85.42% examples, 769548 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:20,382 :INFO :EPOCH 23 - PROGRESS: at 91.53% examples, 769907 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:21,387 :INFO :EPOCH 23 - PROGRESS: at 97.88% examples, 771474 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:21,718 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:46:21,726 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:46:21,730 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:46:21,732 :INFO :EPOCH - 23 : training on 17005207 raw words (12690460 effective words) took 16.4s, 771502 effective words/s\n",
      "2020-08-02 16:46:22,737 :INFO :EPOCH 24 - PROGRESS: at 5.94% examples, 750731 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:23,749 :INFO :EPOCH 24 - PROGRESS: at 12.29% examples, 771227 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:24,751 :INFO :EPOCH 24 - PROGRESS: at 18.64% examples, 781561 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:25,769 :INFO :EPOCH 24 - PROGRESS: at 24.99% examples, 784513 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:26,769 :INFO :EPOCH 24 - PROGRESS: at 31.28% examples, 789362 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:27,775 :INFO :EPOCH 24 - PROGRESS: at 37.27% examples, 785172 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:28,782 :INFO :EPOCH 24 - PROGRESS: at 43.62% examples, 787455 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:29,783 :INFO :EPOCH 24 - PROGRESS: at 49.85% examples, 788371 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:30,787 :INFO :EPOCH 24 - PROGRESS: at 55.73% examples, 783904 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:31,794 :INFO :EPOCH 24 - PROGRESS: at 61.96% examples, 784176 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:32,794 :INFO :EPOCH 24 - PROGRESS: at 68.25% examples, 785474 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:33,798 :INFO :EPOCH 24 - PROGRESS: at 74.54% examples, 786529 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:34,799 :INFO :EPOCH 24 - PROGRESS: at 80.95% examples, 787121 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:35,816 :INFO :EPOCH 24 - PROGRESS: at 87.07% examples, 785372 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:36,823 :INFO :EPOCH 24 - PROGRESS: at 92.89% examples, 781897 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:37,828 :INFO :EPOCH 24 - PROGRESS: at 98.77% examples, 779070 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:38,029 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:46:38,035 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:46:38,044 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:46:38,045 :INFO :EPOCH - 24 : training on 17005207 raw words (12691417 effective words) took 16.3s, 778193 effective words/s\n",
      "2020-08-02 16:46:39,056 :INFO :EPOCH 25 - PROGRESS: at 6.11% examples, 768613 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:40,066 :INFO :EPOCH 25 - PROGRESS: at 12.46% examples, 780053 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:41,081 :INFO :EPOCH 25 - PROGRESS: at 18.93% examples, 789218 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:42,085 :INFO :EPOCH 25 - PROGRESS: at 25.04% examples, 785846 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:43,086 :INFO :EPOCH 25 - PROGRESS: at 31.33% examples, 790241 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:44,095 :INFO :EPOCH 25 - PROGRESS: at 37.51% examples, 788978 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:45,097 :INFO :EPOCH 25 - PROGRESS: at 43.56% examples, 786099 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:46,115 :INFO :EPOCH 25 - PROGRESS: at 49.91% examples, 787537 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:47,127 :INFO :EPOCH 25 - PROGRESS: at 56.20% examples, 788308 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:48,135 :INFO :EPOCH 25 - PROGRESS: at 62.49% examples, 788646 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:49,141 :INFO :EPOCH 25 - PROGRESS: at 68.78% examples, 789261 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:50,149 :INFO :EPOCH 25 - PROGRESS: at 75.25% examples, 791075 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:51,165 :INFO :EPOCH 25 - PROGRESS: at 81.66% examples, 790783 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:52,173 :INFO :EPOCH 25 - PROGRESS: at 88.01% examples, 791396 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:53,181 :INFO :EPOCH 25 - PROGRESS: at 94.18% examples, 790341 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:54,152 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:46:54,163 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:46:54,171 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:46:54,174 :INFO :EPOCH - 25 : training on 17005207 raw words (12691129 effective words) took 16.1s, 787109 effective words/s\n",
      "2020-08-02 16:46:55,190 :INFO :EPOCH 26 - PROGRESS: at 4.94% examples, 621837 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:46:56,197 :INFO :EPOCH 26 - PROGRESS: at 11.05% examples, 691052 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:57,197 :INFO :EPOCH 26 - PROGRESS: at 17.17% examples, 719296 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:46:58,200 :INFO :EPOCH 26 - PROGRESS: at 23.46% examples, 738723 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:46:59,205 :INFO :EPOCH 26 - PROGRESS: at 29.81% examples, 753441 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:00,208 :INFO :EPOCH 26 - PROGRESS: at 35.86% examples, 756907 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:01,213 :INFO :EPOCH 26 - PROGRESS: at 42.15% examples, 762466 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:02,226 :INFO :EPOCH 26 - PROGRESS: at 48.44% examples, 766281 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:03,226 :INFO :EPOCH 26 - PROGRESS: at 54.67% examples, 769644 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:47:04,234 :INFO :EPOCH 26 - PROGRESS: at 60.91% examples, 771287 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:05,243 :INFO :EPOCH 26 - PROGRESS: at 67.31% examples, 774417 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:06,244 :INFO :EPOCH 26 - PROGRESS: at 73.54% examples, 776100 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:07,256 :INFO :EPOCH 26 - PROGRESS: at 80.07% examples, 777958 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:08,260 :INFO :EPOCH 26 - PROGRESS: at 86.36% examples, 779163 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:09,271 :INFO :EPOCH 26 - PROGRESS: at 92.77% examples, 780796 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:10,272 :INFO :EPOCH 26 - PROGRESS: at 98.82% examples, 779641 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:10,469 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:47:10,477 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:47:10,488 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:47:10,489 :INFO :EPOCH - 26 : training on 17005207 raw words (12692196 effective words) took 16.3s, 778329 effective words/s\n",
      "2020-08-02 16:47:11,504 :INFO :EPOCH 27 - PROGRESS: at 5.88% examples, 739800 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:47:12,505 :INFO :EPOCH 27 - PROGRESS: at 11.41% examples, 716262 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:13,512 :INFO :EPOCH 27 - PROGRESS: at 16.64% examples, 697288 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:47:14,512 :INFO :EPOCH 27 - PROGRESS: at 22.16% examples, 698185 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:15,517 :INFO :EPOCH 27 - PROGRESS: at 28.22% examples, 712943 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:16,520 :INFO :EPOCH 27 - PROGRESS: at 33.57% examples, 708447 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:17,523 :INFO :EPOCH 27 - PROGRESS: at 38.80% examples, 702403 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:47:18,530 :INFO :EPOCH 27 - PROGRESS: at 44.15% examples, 699053 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:19,530 :INFO :EPOCH 27 - PROGRESS: at 50.44% examples, 710567 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:20,537 :INFO :EPOCH 27 - PROGRESS: at 56.61% examples, 717792 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:21,538 :INFO :EPOCH 27 - PROGRESS: at 62.85% examples, 724542 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:47:22,538 :INFO :EPOCH 27 - PROGRESS: at 68.72% examples, 726375 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:23,553 :INFO :EPOCH 27 - PROGRESS: at 74.37% examples, 725082 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:24,560 :INFO :EPOCH 27 - PROGRESS: at 79.72% examples, 720110 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:25,567 :INFO :EPOCH 27 - PROGRESS: at 84.60% examples, 712957 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:26,570 :INFO :EPOCH 27 - PROGRESS: at 89.48% examples, 707161 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:27,582 :INFO :EPOCH 27 - PROGRESS: at 95.71% examples, 711320 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:28,361 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:47:28,368 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:47:28,377 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:47:28,378 :INFO :EPOCH - 27 : training on 17005207 raw words (12691315 effective words) took 17.9s, 709824 effective words/s\n",
      "2020-08-02 16:47:29,385 :INFO :EPOCH 28 - PROGRESS: at 5.88% examples, 741916 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:30,407 :INFO :EPOCH 28 - PROGRESS: at 11.58% examples, 721363 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:47:31,410 :INFO :EPOCH 28 - PROGRESS: at 17.70% examples, 738604 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:32,414 :INFO :EPOCH 28 - PROGRESS: at 23.81% examples, 747913 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:33,516 :INFO :EPOCH 28 - PROGRESS: at 25.69% examples, 633958 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:34,528 :INFO :EPOCH 28 - PROGRESS: at 27.04% examples, 557742 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:47:35,536 :INFO :EPOCH 28 - PROGRESS: at 30.04% examples, 533132 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:36,539 :INFO :EPOCH 28 - PROGRESS: at 34.10% examples, 531329 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:37,549 :INFO :EPOCH 28 - PROGRESS: at 38.51% examples, 534118 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:38,555 :INFO :EPOCH 28 - PROGRESS: at 43.39% examples, 542402 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:39,568 :INFO :EPOCH 28 - PROGRESS: at 49.15% examples, 559009 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:47:40,569 :INFO :EPOCH 28 - PROGRESS: at 54.79% examples, 572246 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:41,582 :INFO :EPOCH 28 - PROGRESS: at 60.85% examples, 586759 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:42,595 :INFO :EPOCH 28 - PROGRESS: at 66.67% examples, 596882 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:47:43,603 :INFO :EPOCH 28 - PROGRESS: at 72.60% examples, 607086 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:44,606 :INFO :EPOCH 28 - PROGRESS: at 78.25% examples, 612796 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:45,607 :INFO :EPOCH 28 - PROGRESS: at 83.95% examples, 619128 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:46,610 :INFO :EPOCH 28 - PROGRESS: at 89.77% examples, 625656 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:47,618 :INFO :EPOCH 28 - PROGRESS: at 95.65% examples, 631367 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:48,347 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:47:48,352 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:47:48,354 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:47:48,356 :INFO :EPOCH - 28 : training on 17005207 raw words (12691113 effective words) took 20.0s, 635425 effective words/s\n",
      "2020-08-02 16:47:49,369 :INFO :EPOCH 29 - PROGRESS: at 5.76% examples, 722454 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:50,372 :INFO :EPOCH 29 - PROGRESS: at 11.99% examples, 751893 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:47:51,379 :INFO :EPOCH 29 - PROGRESS: at 18.17% examples, 760548 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:47:52,383 :INFO :EPOCH 29 - PROGRESS: at 24.16% examples, 760554 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:53,401 :INFO :EPOCH 29 - PROGRESS: at 30.16% examples, 759518 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:54,421 :INFO :EPOCH 29 - PROGRESS: at 36.04% examples, 756520 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:55,422 :INFO :EPOCH 29 - PROGRESS: at 42.27% examples, 761355 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:56,425 :INFO :EPOCH 29 - PROGRESS: at 48.21% examples, 760633 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:57,432 :INFO :EPOCH 29 - PROGRESS: at 53.97% examples, 757296 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:58,434 :INFO :EPOCH 29 - PROGRESS: at 59.91% examples, 757068 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:47:59,447 :INFO :EPOCH 29 - PROGRESS: at 66.26% examples, 760590 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:00,451 :INFO :EPOCH 29 - PROGRESS: at 72.43% examples, 762440 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:01,453 :INFO :EPOCH 29 - PROGRESS: at 78.66% examples, 763328 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:02,462 :INFO :EPOCH 29 - PROGRESS: at 85.01% examples, 765608 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:03,470 :INFO :EPOCH 29 - PROGRESS: at 91.30% examples, 767443 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:48:04,473 :INFO :EPOCH 29 - PROGRESS: at 97.59% examples, 768842 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:04,842 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:48:04,843 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:48:04,849 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:48:04,851 :INFO :EPOCH - 29 : training on 17005207 raw words (12692084 effective words) took 16.5s, 769684 effective words/s\n",
      "2020-08-02 16:48:05,866 :INFO :EPOCH 30 - PROGRESS: at 6.29% examples, 785594 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:48:06,868 :INFO :EPOCH 30 - PROGRESS: at 12.52% examples, 785006 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:07,874 :INFO :EPOCH 30 - PROGRESS: at 18.87% examples, 789958 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:08,892 :INFO :EPOCH 30 - PROGRESS: at 25.22% examples, 790911 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:09,896 :INFO :EPOCH 30 - PROGRESS: at 31.45% examples, 792427 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:10,913 :INFO :EPOCH 30 - PROGRESS: at 37.74% examples, 792342 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:11,916 :INFO :EPOCH 30 - PROGRESS: at 44.09% examples, 794062 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:48:12,923 :INFO :EPOCH 30 - PROGRESS: at 50.38% examples, 794533 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:13,930 :INFO :EPOCH 30 - PROGRESS: at 56.73% examples, 795671 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:14,936 :INFO :EPOCH 30 - PROGRESS: at 62.96% examples, 794931 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:15,946 :INFO :EPOCH 30 - PROGRESS: at 69.37% examples, 795924 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:16,953 :INFO :EPOCH 30 - PROGRESS: at 75.72% examples, 795728 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:17,954 :INFO :EPOCH 30 - PROGRESS: at 82.01% examples, 795155 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:18,963 :INFO :EPOCH 30 - PROGRESS: at 88.30% examples, 794892 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:19,969 :INFO :EPOCH 30 - PROGRESS: at 94.71% examples, 795473 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:20,808 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:48:20,814 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:48:20,816 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:48:20,817 :INFO :EPOCH - 30 : training on 17005207 raw words (12689794 effective words) took 16.0s, 795031 effective words/s\n",
      "2020-08-02 16:48:21,830 :INFO :EPOCH 31 - PROGRESS: at 6.06% examples, 760026 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:22,836 :INFO :EPOCH 31 - PROGRESS: at 11.82% examples, 740710 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:23,850 :INFO :EPOCH 31 - PROGRESS: at 17.64% examples, 736272 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:48:24,858 :INFO :EPOCH 31 - PROGRESS: at 22.57% examples, 707414 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:25,866 :INFO :EPOCH 31 - PROGRESS: at 27.69% examples, 696220 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:26,867 :INFO :EPOCH 31 - PROGRESS: at 33.39% examples, 702127 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:48:27,887 :INFO :EPOCH 31 - PROGRESS: at 39.62% examples, 713187 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:28,900 :INFO :EPOCH 31 - PROGRESS: at 45.80% examples, 721049 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:29,907 :INFO :EPOCH 31 - PROGRESS: at 51.62% examples, 722892 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:30,919 :INFO :EPOCH 31 - PROGRESS: at 57.20% examples, 720975 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:48:31,919 :INFO :EPOCH 31 - PROGRESS: at 63.43% examples, 727457 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:32,928 :INFO :EPOCH 31 - PROGRESS: at 69.61% examples, 731554 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:33,937 :INFO :EPOCH 31 - PROGRESS: at 75.90% examples, 735641 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:34,946 :INFO :EPOCH 31 - PROGRESS: at 82.13% examples, 738421 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:35,965 :INFO :EPOCH 31 - PROGRESS: at 88.42% examples, 741589 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:48:36,965 :INFO :EPOCH 31 - PROGRESS: at 94.65% examples, 744395 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:37,790 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:48:37,792 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:48:37,801 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:48:37,803 :INFO :EPOCH - 31 : training on 17005207 raw words (12690262 effective words) took 17.0s, 747377 effective words/s\n",
      "2020-08-02 16:48:38,814 :INFO :EPOCH 32 - PROGRESS: at 6.23% examples, 781355 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:39,834 :INFO :EPOCH 32 - PROGRESS: at 12.76% examples, 794067 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:40,835 :INFO :EPOCH 32 - PROGRESS: at 18.93% examples, 789986 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:41,845 :INFO :EPOCH 32 - PROGRESS: at 25.16% examples, 788956 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:42,856 :INFO :EPOCH 32 - PROGRESS: at 31.28% examples, 786814 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:43,865 :INFO :EPOCH 32 - PROGRESS: at 37.57% examples, 788473 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:44,871 :INFO :EPOCH 32 - PROGRESS: at 43.62% examples, 785335 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:45,877 :INFO :EPOCH 32 - PROGRESS: at 49.85% examples, 786006 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:46,878 :INFO :EPOCH 32 - PROGRESS: at 56.08% examples, 787127 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:47,893 :INFO :EPOCH 32 - PROGRESS: at 62.43% examples, 787878 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:48:48,901 :INFO :EPOCH 32 - PROGRESS: at 68.72% examples, 788350 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:49,906 :INFO :EPOCH 32 - PROGRESS: at 75.13% examples, 789928 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:48:50,907 :INFO :EPOCH 32 - PROGRESS: at 81.42% examples, 789482 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:51,920 :INFO :EPOCH 32 - PROGRESS: at 87.77% examples, 789888 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:52,925 :INFO :EPOCH 32 - PROGRESS: at 94.18% examples, 791002 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:53,839 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:48:53,842 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:48:53,847 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:48:53,848 :INFO :EPOCH - 32 : training on 17005207 raw words (12691408 effective words) took 16.0s, 791220 effective words/s\n",
      "2020-08-02 16:48:54,855 :INFO :EPOCH 33 - PROGRESS: at 6.23% examples, 784307 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:55,866 :INFO :EPOCH 33 - PROGRESS: at 12.58% examples, 787975 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:56,873 :INFO :EPOCH 33 - PROGRESS: at 18.99% examples, 794233 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:57,873 :INFO :EPOCH 33 - PROGRESS: at 25.34% examples, 797880 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:48:58,884 :INFO :EPOCH 33 - PROGRESS: at 31.57% examples, 796841 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:48:59,890 :INFO :EPOCH 33 - PROGRESS: at 37.86% examples, 797480 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:00,893 :INFO :EPOCH 33 - PROGRESS: at 44.03% examples, 795308 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:01,907 :INFO :EPOCH 33 - PROGRESS: at 50.44% examples, 796901 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:02,915 :INFO :EPOCH 33 - PROGRESS: at 56.79% examples, 797625 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:03,920 :INFO :EPOCH 33 - PROGRESS: at 63.02% examples, 796747 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:04,930 :INFO :EPOCH 33 - PROGRESS: at 69.43% examples, 797608 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:05,938 :INFO :EPOCH 33 - PROGRESS: at 75.72% examples, 796643 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:49:06,955 :INFO :EPOCH 33 - PROGRESS: at 82.25% examples, 797346 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:07,960 :INFO :EPOCH 33 - PROGRESS: at 88.48% examples, 796633 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:08,969 :INFO :EPOCH 33 - PROGRESS: at 94.94% examples, 797479 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:09,760 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:49:09,765 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:49:09,769 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:49:09,770 :INFO :EPOCH - 33 : training on 17005207 raw words (12691502 effective words) took 15.9s, 797366 effective words/s\n",
      "2020-08-02 16:49:10,777 :INFO :EPOCH 34 - PROGRESS: at 6.29% examples, 793308 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:11,782 :INFO :EPOCH 34 - PROGRESS: at 12.46% examples, 783494 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:12,787 :INFO :EPOCH 34 - PROGRESS: at 18.93% examples, 793924 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:13,792 :INFO :EPOCH 34 - PROGRESS: at 25.22% examples, 794775 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:14,809 :INFO :EPOCH 34 - PROGRESS: at 31.57% examples, 796545 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:15,814 :INFO :EPOCH 34 - PROGRESS: at 37.80% examples, 796054 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:16,828 :INFO :EPOCH 34 - PROGRESS: at 44.15% examples, 796117 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:17,831 :INFO :EPOCH 34 - PROGRESS: at 50.56% examples, 798689 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:18,839 :INFO :EPOCH 34 - PROGRESS: at 56.85% examples, 798239 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:19,842 :INFO :EPOCH 34 - PROGRESS: at 63.14% examples, 798294 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:20,855 :INFO :EPOCH 34 - PROGRESS: at 69.49% examples, 798138 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:21,863 :INFO :EPOCH 34 - PROGRESS: at 75.96% examples, 798866 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:49:22,875 :INFO :EPOCH 34 - PROGRESS: at 82.30% examples, 798065 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:23,892 :INFO :EPOCH 34 - PROGRESS: at 88.59% examples, 797195 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:49:24,896 :INFO :EPOCH 34 - PROGRESS: at 94.89% examples, 796849 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:25,687 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:49:25,695 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:49:25,702 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:49:25,704 :INFO :EPOCH - 34 : training on 17005207 raw words (12692938 effective words) took 15.9s, 796931 effective words/s\n",
      "2020-08-02 16:49:26,717 :INFO :EPOCH 35 - PROGRESS: at 6.23% examples, 780209 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:27,718 :INFO :EPOCH 35 - PROGRESS: at 12.52% examples, 786087 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:28,720 :INFO :EPOCH 35 - PROGRESS: at 18.87% examples, 791908 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:29,729 :INFO :EPOCH 35 - PROGRESS: at 24.93% examples, 784834 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:30,738 :INFO :EPOCH 35 - PROGRESS: at 30.81% examples, 777783 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:31,751 :INFO :EPOCH 35 - PROGRESS: at 37.10% examples, 780635 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:32,753 :INFO :EPOCH 35 - PROGRESS: at 43.39% examples, 783199 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:33,763 :INFO :EPOCH 35 - PROGRESS: at 49.68% examples, 784665 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:34,772 :INFO :EPOCH 35 - PROGRESS: at 55.97% examples, 786079 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:35,782 :INFO :EPOCH 35 - PROGRESS: at 62.26% examples, 786527 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:36,790 :INFO :EPOCH 35 - PROGRESS: at 68.67% examples, 788468 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:37,797 :INFO :EPOCH 35 - PROGRESS: at 74.90% examples, 788237 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:38,810 :INFO :EPOCH 35 - PROGRESS: at 81.42% examples, 789301 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:39,811 :INFO :EPOCH 35 - PROGRESS: at 87.65% examples, 789351 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:40,826 :INFO :EPOCH 35 - PROGRESS: at 94.12% examples, 790452 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:41,750 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:49:41,757 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:49:41,765 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:49:41,767 :INFO :EPOCH - 35 : training on 17005207 raw words (12690521 effective words) took 16.1s, 790319 effective words/s\n",
      "2020-08-02 16:49:42,774 :INFO :EPOCH 36 - PROGRESS: at 6.29% examples, 792534 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:43,789 :INFO :EPOCH 36 - PROGRESS: at 12.35% examples, 772269 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:44,795 :INFO :EPOCH 36 - PROGRESS: at 18.22% examples, 762034 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:45,808 :INFO :EPOCH 36 - PROGRESS: at 24.57% examples, 770741 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:49:46,809 :INFO :EPOCH 36 - PROGRESS: at 30.86% examples, 778014 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:47,826 :INFO :EPOCH 36 - PROGRESS: at 37.04% examples, 778147 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:49:48,828 :INFO :EPOCH 36 - PROGRESS: at 42.97% examples, 774767 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:49,835 :INFO :EPOCH 36 - PROGRESS: at 49.15% examples, 775566 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:50,846 :INFO :EPOCH 36 - PROGRESS: at 55.26% examples, 775055 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:51,848 :INFO :EPOCH 36 - PROGRESS: at 61.38% examples, 775241 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:49:52,864 :INFO :EPOCH 36 - PROGRESS: at 67.78% examples, 777485 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:49:53,871 :INFO :EPOCH 36 - PROGRESS: at 73.84% examples, 776558 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:54,881 :INFO :EPOCH 36 - PROGRESS: at 80.19% examples, 776884 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:49:55,889 :INFO :EPOCH 36 - PROGRESS: at 86.60% examples, 779058 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:56,899 :INFO :EPOCH 36 - PROGRESS: at 92.89% examples, 779736 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:57,922 :INFO :EPOCH 36 - PROGRESS: at 99.35% examples, 780908 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:49:58,014 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:49:58,019 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:49:58,027 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:49:58,029 :INFO :EPOCH - 36 : training on 17005207 raw words (12691155 effective words) took 16.3s, 780700 effective words/s\n",
      "2020-08-02 16:49:59,037 :INFO :EPOCH 37 - PROGRESS: at 5.88% examples, 741763 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:00,053 :INFO :EPOCH 37 - PROGRESS: at 12.29% examples, 768205 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:01,059 :INFO :EPOCH 37 - PROGRESS: at 18.58% examples, 775830 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:02,069 :INFO :EPOCH 37 - PROGRESS: at 24.75% examples, 776323 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:50:03,088 :INFO :EPOCH 37 - PROGRESS: at 31.10% examples, 781378 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:04,090 :INFO :EPOCH 37 - PROGRESS: at 37.33% examples, 783754 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:05,093 :INFO :EPOCH 37 - PROGRESS: at 43.15% examples, 777414 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:06,099 :INFO :EPOCH 37 - PROGRESS: at 49.38% examples, 778944 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:07,118 :INFO :EPOCH 37 - PROGRESS: at 55.79% examples, 781702 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:08,123 :INFO :EPOCH 37 - PROGRESS: at 62.08% examples, 783070 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 16:50:09,128 :INFO :EPOCH 37 - PROGRESS: at 68.43% examples, 784923 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:10,132 :INFO :EPOCH 37 - PROGRESS: at 74.60% examples, 784728 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:11,138 :INFO :EPOCH 37 - PROGRESS: at 80.83% examples, 783422 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:12,146 :INFO :EPOCH 37 - PROGRESS: at 86.89% examples, 781903 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:13,150 :INFO :EPOCH 37 - PROGRESS: at 93.24% examples, 783220 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:14,159 :INFO :EPOCH 37 - PROGRESS: at 99.53% examples, 783506 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:14,220 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:50:14,226 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:50:14,227 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:50:14,229 :INFO :EPOCH - 37 : training on 17005207 raw words (12690939 effective words) took 16.2s, 783630 effective words/s\n",
      "2020-08-02 16:50:15,244 :INFO :EPOCH 38 - PROGRESS: at 6.35% examples, 791575 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:16,258 :INFO :EPOCH 38 - PROGRESS: at 12.35% examples, 769450 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:17,258 :INFO :EPOCH 38 - PROGRESS: at 18.58% examples, 776089 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:18,264 :INFO :EPOCH 38 - PROGRESS: at 24.81% examples, 779114 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:19,265 :INFO :EPOCH 38 - PROGRESS: at 30.86% examples, 778762 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:20,270 :INFO :EPOCH 38 - PROGRESS: at 36.86% examples, 776491 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:21,275 :INFO :EPOCH 38 - PROGRESS: at 43.03% examples, 777220 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:22,275 :INFO :EPOCH 38 - PROGRESS: at 49.21% examples, 778392 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:23,278 :INFO :EPOCH 38 - PROGRESS: at 55.44% examples, 780136 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:24,278 :INFO :EPOCH 38 - PROGRESS: at 61.61% examples, 780683 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:25,285 :INFO :EPOCH 38 - PROGRESS: at 67.90% examples, 781729 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:26,297 :INFO :EPOCH 38 - PROGRESS: at 74.19% examples, 782775 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:27,300 :INFO :EPOCH 38 - PROGRESS: at 80.66% examples, 784031 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:28,313 :INFO :EPOCH 38 - PROGRESS: at 86.89% examples, 783712 words/s, in_qsize 6, out_qsize 0\n",
      "2020-08-02 16:50:29,314 :INFO :EPOCH 38 - PROGRESS: at 93.24% examples, 785148 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:30,314 :INFO :EPOCH 38 - PROGRESS: at 99.47% examples, 785238 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:30,378 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:50:30,389 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:50:30,390 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:50:30,392 :INFO :EPOCH - 38 : training on 17005207 raw words (12691731 effective words) took 16.2s, 785495 effective words/s\n",
      "2020-08-02 16:50:31,400 :INFO :EPOCH 39 - PROGRESS: at 6.11% examples, 768810 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:32,405 :INFO :EPOCH 39 - PROGRESS: at 12.46% examples, 782404 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:33,418 :INFO :EPOCH 39 - PROGRESS: at 18.87% examples, 788934 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:34,421 :INFO :EPOCH 39 - PROGRESS: at 25.28% examples, 795238 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:35,432 :INFO :EPOCH 39 - PROGRESS: at 31.63% examples, 797668 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:36,451 :INFO :EPOCH 39 - PROGRESS: at 38.04% examples, 799042 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:37,455 :INFO :EPOCH 39 - PROGRESS: at 44.33% examples, 798562 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:38,463 :INFO :EPOCH 39 - PROGRESS: at 50.68% examples, 799398 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:39,467 :INFO :EPOCH 39 - PROGRESS: at 56.97% examples, 799135 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:40,481 :INFO :EPOCH 39 - PROGRESS: at 63.32% examples, 798952 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:41,483 :INFO :EPOCH 39 - PROGRESS: at 69.61% examples, 798772 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:42,485 :INFO :EPOCH 39 - PROGRESS: at 75.96% examples, 798681 words/s, in_qsize 4, out_qsize 1\n",
      "2020-08-02 16:50:43,494 :INFO :EPOCH 39 - PROGRESS: at 82.30% examples, 798058 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:44,494 :INFO :EPOCH 39 - PROGRESS: at 88.71% examples, 799208 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:45,502 :INFO :EPOCH 39 - PROGRESS: at 94.71% examples, 795943 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:46,329 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:50:46,335 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:50:46,343 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:50:46,344 :INFO :EPOCH - 39 : training on 17005207 raw words (12690862 effective words) took 15.9s, 795774 effective words/s\n",
      "2020-08-02 16:50:47,354 :INFO :EPOCH 40 - PROGRESS: at 6.23% examples, 782462 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:48,355 :INFO :EPOCH 40 - PROGRESS: at 12.70% examples, 798814 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:49,357 :INFO :EPOCH 40 - PROGRESS: at 19.05% examples, 800220 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:50,371 :INFO :EPOCH 40 - PROGRESS: at 25.40% examples, 799657 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-02 16:50:51,375 :INFO :EPOCH 40 - PROGRESS: at 31.75% examples, 802363 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:52,379 :INFO :EPOCH 40 - PROGRESS: at 38.04% examples, 802377 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:53,389 :INFO :EPOCH 40 - PROGRESS: at 44.39% examples, 801771 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:54,390 :INFO :EPOCH 40 - PROGRESS: at 50.62% examples, 801076 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:55,400 :INFO :EPOCH 40 - PROGRESS: at 56.97% examples, 801052 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:56,407 :INFO :EPOCH 40 - PROGRESS: at 63.32% examples, 801192 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:57,417 :INFO :EPOCH 40 - PROGRESS: at 69.66% examples, 800920 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:58,421 :INFO :EPOCH 40 - PROGRESS: at 76.07% examples, 801130 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:50:59,422 :INFO :EPOCH 40 - PROGRESS: at 82.36% examples, 800181 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:51:00,425 :INFO :EPOCH 40 - PROGRESS: at 88.59% examples, 799434 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:51:01,434 :INFO :EPOCH 40 - PROGRESS: at 95.00% examples, 799608 words/s, in_qsize 5, out_qsize 0\n",
      "2020-08-02 16:51:02,212 :INFO :worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-02 16:51:02,224 :INFO :worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-02 16:51:02,225 :INFO :worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-02 16:51:02,227 :INFO :EPOCH - 40 : training on 17005207 raw words (12692096 effective words) took 15.9s, 799346 effective words/s\n",
      "2020-08-02 16:51:02,229 :INFO :training on a 680208280 raw words (507656131 effective words) took 690.4s, 735338 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_data,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20169373, -0.22507694, -0.09382267, -0.00425628,  0.0994833 ,\n",
       "       -0.02771945,  0.30235487, -0.02431474,  0.32105815,  0.1743709 ,\n",
       "       -0.09962266,  0.08622926,  0.21271455, -0.10941842,  0.17956604,\n",
       "        0.2929854 , -0.15737611, -0.20132913, -0.04100657,  0.10605428,\n",
       "       -0.05084371,  0.11696529, -0.03186974, -0.00066347,  0.15038636,\n",
       "       -0.23955227,  0.05984666, -0.16224216, -0.03229687,  0.11278835,\n",
       "        0.2681371 , -0.13419876, -0.08148566, -0.0548914 ,  0.26457947,\n",
       "       -0.06902724,  0.2113031 ,  0.03951471,  0.25166818, -0.13541122,\n",
       "       -0.05450862, -0.02158979,  0.08941766,  0.08173756,  0.03355532,\n",
       "       -0.04006654, -0.43827912,  0.18767165,  0.01921168, -0.10870411],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to get document vector of a sentence , pass the sentence in line of list of words to the infer_vector method\n",
    "model.infer_vector(['Corona','virus','pandemic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:brown'>Compute similarity metrics like cosine similarity and soft cosine similarity</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import softcossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_1 = 'Sachine is a cricket player and a opening batsman'.split()\n",
    "sent_2 ='Dhoni is a cricket player too He is a batsman and keeper'.split()\n",
    "sent_3 = 'Anand is a chess player'.split()\n",
    "sent_4 = 'Who killed Sushant Singh Rajput'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 20:12:05,123 :INFO :constructing a term similarity matrix\n",
      "2020-08-02 20:12:05,126 :INFO :PROGRESS: at 5.26% rows (1 / 19, 0 skipped, 5.263158% density)\n",
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "2020-08-02 20:12:05,317 :INFO :constructed a term similarity matrix with 87.257618 % nonzero elements\n"
     ]
    }
   ],
   "source": [
    "#Prepre the similarity matrix\n",
    "similarity_matrix =fasttext_model1300.similarity_matrix(dictionary,tfidf=None,threshold=0.0,exponent=2.0,nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents=[sent_1,sent_2,sent_3,sent_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-02 20:12:10,508 :INFO :adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-08-02 20:12:10,511 :INFO :built Dictionary(19 unique tokens: ['Sachine', 'a', 'and', 'batsman', 'cricket']...) from 4 documents (total 31 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dictionary=corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert bag of words\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_3 = dictionary.doc2bow(sent_3)\n",
    "sent_4 = dictionary.doc2bow(sent_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8477188222533869\n"
     ]
    }
   ],
   "source": [
    "#compute soft cosine Similarity\n",
    "print(softcossim(sent_1,sent_2,similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6847319615715306\n"
     ]
    }
   ],
   "source": [
    "print(softcossim(sent_1,sent_3,similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7099691700919265\n"
     ]
    }
   ],
   "source": [
    "print(softcossim(sent_2,sent_3,similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2127669278652476\n"
     ]
    }
   ],
   "source": [
    "print(softcossim(sent_1,sent_4,similarity_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:green'>Some useful similarities and distance metrics based on word embedding models fasttext, GloVe</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beetroot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "#dissimilar\n",
    "print(fasttext_model1300.doesnt_match(['India','Australia','china','pakistan','beetroot']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22957539558410645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Niraj Kumar\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "#cosine distance between words\n",
    "print(fasttext_model1300.distance('king','queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22957546 0.465837   0.547001  ]\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance between a word and series of words\n",
    "print(fasttext_model1300.distances('king',['queen','man','woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77042454, 0.534163  , 0.45299897, 0.76572555], dtype=float32)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cosine Similarities\n",
    "fasttext_model1300.cosine_similarities(fasttext_model1300['king'],vectors_all=(fasttext_model1300['queen'],fasttext_model1300['man'],\n",
    "                                                                              fasttext_model1300['woman'],\n",
    "                                                                               fasttext_model1300['queen']+fasttext_model1300['man']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the words closer to w1 than w2\n",
    "print(glov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
